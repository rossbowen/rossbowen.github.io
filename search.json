[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello!",
    "section": "",
    "text": "{\n  function binomialRandom(n, p) {\n    let x = 0;\n    for (let i = 0; i &lt; n; i++) {\n      if (Math.random() &lt; p) {\n        x++;\n      }\n    }\n    return x;\n  }\n\n  function generateData(particles, places) {\n    const data = [];\n\n    particles.forEach((value, index) =&gt; {\n      for (let y = 1; y &lt;= value; y++) {\n        const colour = `hsl(${Math.floor(Math.random() * 360)}, 70%, 70%)`;\n        data.push({ x: index - Math.floor(places / 2), y: y, colour: colour });\n      }\n    });\n\n    return data;\n  }\n\n  function leap(data, places, maxParticles) {\n    const maxYs = data.reduce((acc, p) =&gt; {\n      if (!acc[p.x] || p.y &gt; acc[p.x].y) {\n        acc[p.x] = p;\n      }\n      return acc;\n    }, {});\n\n    const topmostParticles = Object.values(maxYs);\n    const particleToModifyIndex = Math.floor(\n      Math.random() * topmostParticles.length\n    );\n    const particleToModify = topmostParticles[particleToModifyIndex];\n\n    const minX = Math.ceil(-(places + 1) / 2);\n    const maxX = Math.floor((places + 1) / 2);\n\n    let newX;\n    if (particleToModify.x === minX) {\n      newX = particleToModify.x + 1;\n    } else if (particleToModify.x === maxX) {\n      newX = particleToModify.x - 1;\n    } else {\n      newX = particleToModify.x + (Math.random() &lt; 0.5 ? -1 : 1);\n      if (newX &lt; minX) newX = minX;\n      if (newX &gt; maxX) newX = maxX;\n    }\n\n    const maxYAtNewX =\n      Math.max(0, ...data.filter((p) =&gt; p.x === newX).map((p) =&gt; p.y)) + 1;\n    if (maxYAtNewX &gt; maxParticles) {\n      return data;\n    }\n\n    data.forEach((p) =&gt; {\n      if (p === particleToModify) {\n        p.x = newX;\n        p.y = maxYAtNewX;\n      }\n    });\n\n    return data;\n  }\n\n  function createSvg(width, height, margin) {\n    return d3.create(\"svg\").attr(\"viewBox\", [0, 0, width, height]);\n  }\n\n  function createScales(places, maxParticles, width, height, margin) {\n    const x = d3\n      .scaleLinear()\n      .domain([-(places + 1) / 2, (places + 1) / 2])\n      .range([margin.left, width - margin.right]);\n\n    const y = d3\n      .scaleLinear()\n      .domain([0, maxParticles + 1])\n      .range([height - margin.bottom, margin.top]);\n\n    return { x, y };\n  }\n\n  function createXAxis(g, x, height, margin, places) {\n    g.attr(\"transform\", `translate(0,${height - margin.bottom})`).call(\n      d3.axisBottom(x).ticks(places).tickSizeOuter(0)\n    );\n  }\n\n  function drawParticles(svg, data, x, y, radius) {\n    return svg\n      .append(\"g\")\n      .selectAll(\"circle\")\n      .data(data)\n      .join(\"circle\")\n      .attr(\"cx\", (d) =&gt; x(d.x))\n      .attr(\"cy\", (d) =&gt; y(d.y))\n      .attr(\"r\", radius)\n      .attr(\"fill\", (d) =&gt; d.colour);\n  }\n\n  function animate(data, places, maxParticles, circles, x, y, duration) {\n    setInterval(() =&gt; {\n      const newData = leap(data, places, maxParticles);\n\n      circles = circles.data(newData).join(\n        (enter) =&gt;\n          enter\n            .append(\"circle\")\n            .attr(\"cx\", (d) =&gt; x(d.x))\n            .attr(\"cy\", (d) =&gt; y(d.y))\n            .attr(\"fill\", (d) =&gt; d.colour)\n            .call((enter) =&gt; enter.transition().duration(duration)),\n        (update) =&gt;\n          update.call((update) =&gt;\n            update\n              .transition()\n              .duration(duration)\n              .attr(\"cx\", (d) =&gt; x(d.x))\n              .attr(\"cy\", (d) =&gt; y(d.y))\n              .attr(\"fill\", (d) =&gt; d.colour)\n          ),\n        (exit) =&gt;\n          exit.call((exit) =&gt; exit.transition().duration(duration).remove())\n      );\n    }, duration);\n  }\n\n  function main() {\n    const width = window.innerWidth &lt; 768 ? 300 : 400;\n    const height = 100\n    const places = 10;\n    const maxParticles = 3;\n    const radius = height / 25;\n    const margin = { top: 20, right: 30, bottom: 40, left: 40 };\n    const p = 0.4;\n    const duration = 500;\n\n    const particles = Array.from({ length: places + 1 }, () =&gt;\n      binomialRandom(maxParticles, p)\n    );\n    const data = generateData(particles, places);\n\n    const svg = createSvg(width, height, margin);\n    const { x, y } = createScales(places, maxParticles, width, height, margin);\n    svg.append(\"g\").call((g) =&gt; createXAxis(g, x, height, margin, places));\n\n    let circles = drawParticles(svg, data, x, y, radius);\n    animate(data, places, maxParticles, circles, x, y, duration);\n\n    return svg.node();\n  }\n\n  return main();\n}\n\n\n\n\n\n\n\nHello!\nI‚Äôm Ross. I‚Äôm Head of Analytics at Companies House üìàüè°üìä.\nI like Bayesian statistics, multilevel models and R. I think confidence intervals are overrated and misunderstood.\nHere‚Äôs some of my recent stuff:\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n2024-05-28\n\n\nExploring expert systems\n\n\n\n\n2023-10-31\n\n\nHow long will my machine last?\n\n\n\n\n2023-09-06\n\n\nConfidence intervals in the wild\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/exploring-expert-systems/index.html",
    "href": "blog/exploring-expert-systems/index.html",
    "title": "Exploring expert systems",
    "section": "",
    "text": "I‚Äôve been reading Data Patterns recently and I caught a line about trying to build a causal model of a business in order to understand it better.\nThis idea got me thinking about causal inference and I recalled a toy expert system used to determine if a patient has dyspnoea (shortness of breath). I found the original paper and decided to replicate the results using Stan.\nThe idea behind the system is:\nThis description leads to a causal Directed Acyclic Graph (DAG) that shows the dependencies among different traits.\nflowchart\n    A[visit to Asia?] --&gt; T[tuberculosis?]\n    S[smoking?] --&gt; L[lung cancer?]\n    S --&gt; B[bronchitis?]\n    T --&gt; E[either tuberculosis or lung cancer?]\n    L --&gt; E\n    E --&gt; X[positive X-ray?]\n    E --&gt; D[dyspnoea?]\n    B --&gt; D\nThese DAGs help us understand the dependencies between different traits and ask interesting questions about the system. The paper mentions several scenarios:\nWe aim to simulate data using the model and relationships described in the paper. From these samples, we can estimate the probabilities for each of the questions posed."
  },
  {
    "objectID": "blog/exploring-expert-systems/index.html#model-specification",
    "href": "blog/exploring-expert-systems/index.html#model-specification",
    "title": "Exploring expert systems",
    "section": "Model specification",
    "text": "Model specification\nThe paper provides several conditional probabilities:\n\nThe probability of someone visiting Asia is 0.01, and the probability of someone smoking is 0.5.\n\n\\[ P(\\text{asia} = 1) = 0.01 \\]\n\\[ P(\\text{smoking} = 1) = 0.5 \\]\n\nIf someone has visited Asia, the probability of tuberculosis is 0.05. If they have not visited Asia, the probability is 0.01.\n\n\\[ P(\\text{tuberculosis} = 1 \\mid \\text{asia} = 1) = 0.05 \\] \\[ P(\\text{tuberculosis} = 1 \\mid \\text{asia} = 0) = 0.01 \\]\n\nIf someone smokes, the probability of lung cancer is 0.1. If they do not smoke, the probability is 0.01.\n\n\\[ P(\\text{lung cancer} = 1 \\mid \\text{smoking} = 1) = 0.1 \\] \\[ P(\\text{lung cancer} = 1 \\mid \\text{smoking} = 0) = 0.01 \\]\n\nIf someone smokes, the probability of bronchitis is 0.6. If they do not smoke, the probability is 0.3.\n\n\\[ P(\\text{bronchitis} = 1 \\mid \\text{smoking} = 1) = 0.6 \\] \\[ P(\\text{bronchitis} = 1\\mid \\text{smoking} = 0) = 0.3 \\]\n\nHaving either tuberculosis or lung cancer is represented by the event ‚Äúeither‚Äù.\n\n\\[ \\{ \\text{either} = 1 \\} = \\{ \\text{tuberculosis} = 1 \\} \\vee \\{ \\text{lung cancer} = 1 \\} \\]\n\nIf someone has either tuberculosis or lung cancer, the probability of a positive x-ray is 0.98. If they do not have either, the probability is 0.05. This can be interpreted as the x-ray having 98% sensitivity and 95% specificity.\n\n\\[ P(\\text{x-ray} = 1 \\mid \\text{either} = 1) = 0.98 \\] \\[ P(\\text{x-ray} = 1 \\mid \\text{either} = 0) = 0.05 \\]\n\nFinally, if someone has either tuberculosis or lung cancer and also has bronchitis, the probability of dyspnoea is 0.9. If they have either but not bronchitis, the probability is 0.7. If they do not have either but do have bronchitis, the probability is 0.8. If they have neither and do not have bronchitis, the probability is 0.1.\n\n\\[ P(\\text{dyspnoea} = 1 \\mid \\text{either} = 1, \\text{bronchitis} = 1) = 0.9 \\] \\[ P(\\text{dyspnoea} = 1 \\mid \\text{either} = 1, \\text{bronchitis} = 0) = 0.7 \\] \\[ P(\\text{dyspnoea} = 1 \\mid \\text{either} = 0, \\text{bronchitis} = 1) = 0.8 \\] \\[ P(\\text{dyspnoea} = 1 \\mid \\text{either} = 0, \\text{bronchitis} = 0) = 0.1 \\]"
  },
  {
    "objectID": "blog/exploring-expert-systems/index.html#simulating",
    "href": "blog/exploring-expert-systems/index.html#simulating",
    "title": "Exploring expert systems",
    "section": "Simulating",
    "text": "Simulating\nI remembered an old WinBUGS simulation which used this model. The WinBUGS documentation was hard to find, but luckily @chjackson has mirrored it on GitHub. Unfortunately, this example is missing from the example models Stan repository.\nThe WinBUGS simulation chooses to specify each trait as a categorical variable with two categories, instead of modelling them as Bernoulli random variables - I‚Äôd guess to make it easier to specify the conditional probabilities.\nmodel\n{\n    smoking ~ dcat(p.smoking[1:2])\n    tuberculosis ~ dcat(p.tuberculosis[asia,1:2])\n    lung.cancer ~ dcat(p.lung.cancer[smoking,1:2])\n    bronchitis ~ dcat(p.bronchitis[smoking,1:2])\n    either &lt;- max(tuberculosis,lung.cancer)\n    xray ~ dcat(p.xray[either,1:2])\n    dyspnoea ~ dcat(p.dyspnoea[either,bronchitis,1:2])\n}\nIn this example, all parameters are known and included as data, so there are no unknown probabilistic parameters like in a typical Stan simulation. The Stan guidance has a section on sampling without parameters which shows how we can use a generated quantities block to create data based on the model.\n\ndata {\n  real&lt;lower=0, upper=1&gt; p_asia;                  // 0.01\n  real&lt;lower=0, upper=1&gt; p_smoking;               // 0.5\n\n  // The following are the conditional probabilities\n  array[2] real&lt;lower=0, upper=1&gt; p_tuberculosis; // 0.01, 0.05\n  array[2] real&lt;lower=0, upper=1&gt; p_lung_cancer;  // 0.01, 0.1\n  array[2] real&lt;lower=0, upper=1&gt; p_bronchitis;   // 0.3, 0.6\n  array[2] real&lt;lower=0, upper=1&gt; p_xray;         // 0.05, 0.98\n  array[2, 2] real&lt;lower=0, upper=1&gt; p_dyspnoea;  // 0.1, 0.8, 0.7, 0.9\n}\n\ngenerated quantities {\n  int&lt;lower=0, upper=1&gt; asia = bernoulli_rng(p_asia);\n  int&lt;lower=0, upper=1&gt; smoking = bernoulli_rng(p_smoking);\n  int&lt;lower=0, upper=1&gt; tuberculosis = bernoulli_rng(p_tuberculosis[asia + 1]);\n  int&lt;lower=0, upper=1&gt; lung_cancer = bernoulli_rng(p_lung_cancer[smoking + 1]);\n  int&lt;lower=0, upper=1&gt; bronchitis = bernoulli_rng(p_bronchitis[smoking + 1]);\n  int&lt;lower=0, upper=1&gt; either = max(tuberculosis, lung_cancer);\n  int&lt;lower=0, upper=1&gt; xray = bernoulli_rng(p_xray[either + 1]);\n  int&lt;lower=0, upper=1&gt; dyspnoea = bernoulli_rng(p_dyspnoea[either + 1, bronchitis + 1]);\n}\n\nWe use indexing to retrieve the appropriate conditional probability at each step. For example, if asia == 0 then tuberculosis = bernoulli_rng(p_tuberculosis[1]), but if asia == 1 then tuberculosis = bernoulli_rng(p_tuberculosis[2]).\nWe run the model with rstan to generate our samples.\n\nlibrary(rstan)\nlibrary(dplyr)\nlibrary(tidyr)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nset.seed(123)\n\ndata_list &lt;- list(\n  p_asia         = 0.01,\n  p_smoking      = 0.5,\n  p_tuberculosis = c(0.01, 0.05),\n  p_lung_cancer  = c(0.01, 0.1),\n  p_bronchitis   = c(0.3, 0.6),\n  p_xray         = c(0.05, 0.98),\n  p_dyspnoea     = matrix(c(0.1, 0.8, 0.7, 0.9), nrow = 2, byrow = TRUE)\n)\n\nfit &lt;- sampling(\n  model,\n  data = data_list,\n  iter = 100000,\n  chains = 4,\n  algorithm = \"Fixed_param\" # Needed for sampling without parameters\n)\n\nprint(fit)\n#&gt; Inference for Stan model: anon_model.\n#&gt; 4 chains, each with iter=1e+05; warmup=50000; thin=1; \n#&gt; post-warmup draws per chain=50000, total post-warmup draws=2e+05.\n#&gt; \n#&gt;              mean se_mean   sd 2.5% 25% 50% 75% 97.5%  n_eff Rhat\n#&gt; asia         0.01       0 0.10    0   0   0   0     0 198070    1\n#&gt; smoking      0.50       0 0.50    0   0   1   1     1 200360    1\n#&gt; tuberculosis 0.01       0 0.10    0   0   0   0     0 199880    1\n#&gt; lung_cancer  0.06       0 0.23    0   0   0   0     1 195044    1\n#&gt; bronchitis   0.45       0 0.50    0   0   0   1     1 198390    1\n#&gt; either       0.07       0 0.25    0   0   0   0     1 196256    1\n#&gt; xray         0.11       0 0.31    0   0   0   0     1 198948    1\n#&gt; dyspnoea     0.44       0 0.50    0   0   0   1     1 199239    1\n#&gt; lp__         0.00     NaN 0.00    0   0   0   0     0    NaN  NaN\n#&gt; \n#&gt; Samples were drawn using (diag_e) at Mon Jun  3 23:07:42 2024.\n#&gt; For each parameter, n_eff is a crude measure of effective sample size,\n#&gt; and Rhat is the potential scale reduction factor on split chains (at \n#&gt; convergence, Rhat=1).\n\nThe simulation ran successfully, and the mean column of the summary gives the marginal probabilities for each of the traits in the system. Let‚Äôs take a look at our samples:\n\nparams &lt;- rstan::extract(fit)\nsamples &lt;- tibble::as_tibble(params)\n\nprint(samples)\n#&gt; # A tibble: 200,000 √ó 9\n#&gt;     asia smoking tuberculosis lung_cancer bronchitis either  xray dyspnoea  lp__\n#&gt;    &lt;dbl&gt; &lt;dbl[1&gt;    &lt;dbl[1d]&gt;   &lt;dbl[1d]&gt;  &lt;dbl[1d]&gt; &lt;dbl[&gt; &lt;dbl&gt; &lt;dbl[1d&gt; &lt;dbl&gt;\n#&gt;  1     0       0            0           0          0      0     0        0     0\n#&gt;  2     0       1            0           1          1      1     1        1     0\n#&gt;  3     0       0            0           0          0      0     0        0     0\n#&gt;  4     0       0            0           0          0      0     0        0     0\n#&gt;  5     0       1            0           0          1      0     0        1     0\n#&gt;  6     0       1            1           0          0      1     1        1     0\n#&gt;  7     0       1            0           0          1      0     0        1     0\n#&gt;  8     0       0            0           0          1      0     0        1     0\n#&gt;  9     0       0            0           0          0      0     0        0     0\n#&gt; 10     0       1            0           0          1      0     0        1     0\n#&gt; # ‚Ñπ 199,990 more rows"
  },
  {
    "objectID": "blog/exploring-expert-systems/index.html#answering-some-questions",
    "href": "blog/exploring-expert-systems/index.html#answering-some-questions",
    "title": "Exploring expert systems",
    "section": "Answering some questions",
    "text": "Answering some questions\nSo now we have some simulated data, we can estimate probabilities by looking at the proportion of samples which meet certain conditions. First, let‚Äôs do some sense checks to make sure our simulated data gives results we‚Äôd expect from the model specification.\nThe model specification says the probability of someone having visited Asia is 0.01, and the probability that someone smokes is 0.5. Our samples agree with this.\n\nsamples |&gt;\n  summarise(\n    asia_prob = mean(asia),\n    smoking_prob = mean(smoking)\n  )\n#&gt; # A tibble: 1 √ó 2\n#&gt;   asia_prob smoking_prob\n#&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1   0.00952        0.504\n\nTo check another, the model specification says that if someone smokes, the probability they have lung cancer is 0.1. If they do not smoke, the probability is 0.01. Our samples agree with this too, so it looks like our simulation has behaved as expected.\n\nsamples |&gt;\n  group_by(smoking) |&gt;\n  summarise(lung_cancer_prob = mean(lung_cancer))\n#&gt; # A tibble: 2 √ó 2\n#&gt;   smoking lung_cancer_prob\n#&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n#&gt; 1       0          0.00957\n#&gt; 2       1          0.101\n\nNow we can answer the questions from the paper:\n\nA patient presents at a chest clinic with dyspnoea, and has recently visited Asia. Smoking history and chest X-ray are not yet available. The doctor would like to know the chance that each of the diseases is present.\n\nHere we‚Äôll want to filter to samples where asia == 1 and dyspnoea == 1, and to look at the occurrence of each of the other traits.\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1) |&gt;\n  # Make each instance of a disease appear on its own line\n  pivot_longer(cols = c(-asia,-dyspnoea), names_to = \"trait\") |&gt;\n  group_by(trait) |&gt;\n  summarise(probability = mean(value))\n#&gt; # A tibble: 6 √ó 2\n#&gt;   trait        probability\n#&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 bronchitis        0.803 \n#&gt; 2 either            0.176 \n#&gt; 3 lung_cancer       0.0873\n#&gt; 4 smoking           0.643 \n#&gt; 5 tuberculosis      0.0931\n#&gt; 6 xray              0.217\n\nFrom this we can see it‚Äôs likely that the patient has bronchitis, and less likely that they have lung cancer or tuberculosis. It‚Äôs quite likely that the patient smokes, and less likely that a positive x-ray would be seen. The probabilities agree with what was found in the WinBUGS simulation (which are computed by taking one away from the mean in the reported table).\n\n‚Ä¶if tuberculosis were ruled out by another test, how would that change the belief in lung cancer?\n\nHere we filter for samples where tuberculosis == 0 too.\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1, tuberculosis == 0) |&gt;\n  pivot_longer(cols = c(-asia,-dyspnoea), names_to = \"trait\") |&gt;\n  group_by(trait) |&gt;\n  summarise(probability = mean(value)) |&gt;\n  filter(trait == \"lung_cancer\")\n#&gt; # A tibble: 1 √ó 2\n#&gt;   trait       probability\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1 lung_cancer      0.0911\n\nThe probability of lung cancer once we‚Äôve ruled out tuberculosis hasn‚Äôt changed by much.\n\nAlso, would knowing smoking history or getting an X-ray contribute most information about cancer, given that smoking may ‚Äòexplain away‚Äô the dyspnoea since bronchitis is considered a possibility?\n\nGoing back to the case where asia == 1 and dyspnoea == 1, we can look at different cases for smoking history and x-ray results to see how that affects the probability of lung cancer.\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1) |&gt;\n  group_by(smoking) %&gt;%\n  summarise(lung_cancer_prob = mean(lung_cancer))\n#&gt; # A tibble: 2 √ó 2\n#&gt;   smoking lung_cancer_prob\n#&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n#&gt; 1       0           0.0130\n#&gt; 2       1           0.129\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1) |&gt;\n  group_by(xray) %&gt;%\n  summarise(lung_cancer_prob = mean(lung_cancer))\n#&gt; # A tibble: 2 √ó 2\n#&gt;    xray lung_cancer_prob\n#&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n#&gt; 1     0            0    \n#&gt; 2     1            0.403\n\nHere we can see that the x-ray result gives a stronger signal for the presence (or absence) of lung cancer presence than smoking history.\n\nFinally, when all information is in, can we identify which was the most influential in forming our judgement?\n\nSo given asia == 1 and dyspnoea == 1, we can look at the samples for different known smoking histories and different xray results and see what they do to the probabilities for each of the diseases.\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1) |&gt;\n  group_by(smoking, xray) |&gt;\n  summarise(\n    tuberculosis_prob = mean(tuberculosis),\n    lung_cancer_prob = mean(lung_cancer),\n    bronchitis_prob = mean(bronchitis)\n  ) |&gt;\n  arrange(xray)\n#&gt; `summarise()` has grouped output by 'smoking'. You can override using the\n#&gt; `.groups` argument.\n#&gt; # A tibble: 4 √ó 5\n#&gt; # Groups:   smoking [2]\n#&gt;   smoking  xray tuberculosis_prob lung_cancer_prob bronchitis_prob\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1       0     0           0.004             0                0.772\n#&gt; 2       1     0           0.00236           0                0.915\n#&gt; 3       0     1           0.667             0.0702           0.404\n#&gt; 4       1     1           0.310             0.550            0.674\n\nFrom these results, we see that a positive x-ray increases the probability of lung cancer and tuberculosis, regardless of smoking status, while smoking increases bronchitis likelihood regardless of the x-ray result. The x-ray result appears to be the most influential additional information here, as it causes the largest changes to probabilities of the different diseases."
  },
  {
    "objectID": "blog/confidence-intervals-in-the-wild/index.html",
    "href": "blog/confidence-intervals-in-the-wild/index.html",
    "title": "Confidence intervals in the wild",
    "section": "",
    "text": "Confidence intervals have always come up during interviews for analytical roles. I‚Äôm usually asked what a confidence interval is, and I‚Äôll respond along the lines of the definition given by the Office for National Statistics.\nThis definition aligns with that I was taught ‚Äì if we took many samples and created many 95% confidence intervals, we‚Äôd expect 95% of those intervals to contain the true value for the parameter of interest. For many X% confidence intervals, we‚Äôd expect X% of the intervals to contain the true value.\nI‚Äôve then been asked how I would help a non-technical audience interpret a specific confidence interval. The thing is, I‚Äôve never felt that confidence intervals are a good thing to use with that kind of audience. Many statisticians misinterpret them, so how we could expect a non-technical audience to use them correctly?\nI‚Äôve gotten into some awkward debates over this ‚Äì so here‚Äôs me getting my thoughts together."
  },
  {
    "objectID": "blog/confidence-intervals-in-the-wild/index.html#what-does-a-confidence-interval-actually-tell-you",
    "href": "blog/confidence-intervals-in-the-wild/index.html#what-does-a-confidence-interval-actually-tell-you",
    "title": "Confidence intervals in the wild",
    "section": "What does a confidence interval actually tell you?",
    "text": "What does a confidence interval actually tell you?\nI‚Äôd see a confidence interval in the wild and feel a bit unsure of how I could interpret it. If the definition of a confidence interval describes a behavior across many intervals, what does it mean when we‚Äôre presented with a single interval?\nA quick google and we see some common interpretations of confidence intervals:\n\nThere‚Äôs a 95% chance that the parameter of interest lies within a 95% confidence interval.\nConfidence intervals are a range in which we think the true value is likely to lie.\n\nI couldn‚Äôt see how either of these followed from the definition we used earlier, about creating many intervals and on average X% of them containing the true value. I went searching and found many blog posts on the subject from Andrew Gelman, one of which introduced this paper from Richard Morey et al. which has shaped my thoughts over the years.\n\nTake this example from some of the ONS‚Äôs COVID-19 reporting:\n\nThe latest estimated rate of coronavirus (COVID-19) reinfections on 28 October 2022 was 42.8 per 100,000 participant days at risk (95% confidence interval: 42.0 to 43.6).\n\nHere we have a single 95% interval reported alongside an estimate of the COVID-19 reinfection rate. Maybe this is one of the lucky intervals which contains the true reinfection rate, but maybe it‚Äôs not. We don‚Äôt know.\nThe idea behind the 95% confidence interval is to perform some procedure which, on average, returns intervals \\((L, U)\\) which contain the true parameter 95% of the time.\n\\[P(L(X) \\leq \\theta \\leq U(X)) = 0.95\\]\nWe write \\(L(X)\\) and \\(U(X)\\) to emphasise these are random variables which in practice will often depend on the data. The problem occurs when we try to replace \\((L(X), U(X))\\) with an actual interval we‚Äôve obtained.\nIf \\(\\theta\\) was the true COVID-19 reinfection rate, \\(\\theta\\) is just some fixed number we don‚Äôt know the value of. We can‚Äôt say anything probabilistic about the value of \\(\\theta\\) (unless we start using Bayesian techniques).\nBut, if we replace \\(L(X)\\) and \\(U(X)\\) above with the obtained interval, the probability changes.\n\\[\n  P(42.0 \\leq \\theta \\leq 43.6) = \\begin{cases}\n    1, & \\text{if } 42.0 \\leq \\theta \\leq 43.6, \\\\\n    0, & \\text{otherwise}.\n  \\end{cases}\n\\]\nAndrew Gelman provides an analogy in a blog post to explain why the probability changes once you substitute actual values into the interval. To provide a similar analogy ‚Äì let‚Äôs say 170cm is a fairly average male height ‚Äì we could imagine that half of men are taller and half are shorter than 170cm. Bob is 168cm tall.\nIt seems reasonable to say, if \\(X\\) was the height of some arbitrary man, that the probability of that man being taller than 170cm is a half.\n\\[P(X &gt; 170\\text{cm}) = 0.5.\\]\nBut we can‚Äôt just replace \\(X\\), which represents the height of some arbitrary man. with Bob‚Äôs height and expect the probability to remain the same. If we condition the above probability to talk specifically about Bob, rather than some arbitrary man, then the probability becomes 0, as Bob is shorter than 170cm.\n\\[P(X &gt; 170\\text{cm} \\mid X = \\text{Bob's height}) = P(168\\text{cm} &gt; 170\\text{cm}) = 0.\\]\nJerzy Neyman (who introduced confidence intervals in this 1937 paper) was so clear about their interpretation:\n\nIt will be noticed that in the above description the probability statements refer to the problems of estimation with which the statistician will be concerned in the future. In fact, I have repeatedly stated that the frequency of correct results will tend to \\(\\alpha\\). Consider now the case when a sample \\(E'\\), is already drawn, and the calculations have given \\(\\underset{\\bar{}}{\\theta}(E') = 1\\) and \\(\\bar{\\theta}(E') = 2\\). Can we say that in this particular case the probability of the true value falling between 1 and 2 is equal to \\(\\alpha\\)?\nThe answer is obviously in the negative. The parameter \\(\\theta_1\\) is an unknown constant, and no probability statement concerning its value may be made, that is except for hypothetical and trivial ones\n\\[\n\\begin{equation} \\tag{21}\n  P(1 \\leq \\theta_1 \\leq 2) = \\begin{cases}\n    1, & \\text{if } 1 \\leq \\theta_1 \\leq 2, \\\\\n    0, & \\text{if either } \\theta_1 \\leq 2 \\text{ or } 2 \\geq \\theta_1.\n  \\end{cases}\n\\end{equation}\n\\]\n\n\nTo sum up ‚Äì we can‚Äôt say that all confidence intervals give a range of likely values for some parameter, at least not from the definition of a confidence interval alone. Some confidence intervals may be sensible to interpret as a Bayesian credible intervals, at which point we could make statements about likely values.\nOr as Morey et al.¬†put it:\n\nIn the case where data are normally distributed, for instance, there is a particular prior that will lead to a confidence interval that is numerically identical to Bayesian credible intervals computed using the Bayesian posterior (Jeffreys, 1961; Lindley, 1965). This might lead one to suspect that it does not matter whether one uses confidence procedures or Bayesian procedures. We showed, however, that confidence intervals and credible intervals can disagree markedly. The only way to know that a confidence interval is numerically identical to some credible interval is to prove it. The correspondence cannot ‚Äì and should not ‚Äì be assumed."
  },
  {
    "objectID": "blog/confidence-intervals-in-the-wild/index.html#closing-thoughts",
    "href": "blog/confidence-intervals-in-the-wild/index.html#closing-thoughts",
    "title": "Confidence intervals in the wild",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nI think that confidence intervals have a use, particularly when the procedures to create them provide intervals which help describe the uncertainty in an estimate. But I‚Äôd be nervous to include them when writing for non-technical audiences, mainly due to the risk of misinterpretation:\n\nA specific 95% confidence interval doesn‚Äôt cover the true value with 95% probability, but many readers will see an interval and assume that.\nA confidence interval doesn‚Äôt give a range of likely values for some parameter, unless it is appropriate to interpret it as a Bayesian credible interval ‚Äì which a non-technical audience wouldn‚Äôt be familiar with. A reader may assume all intervals provide likely values, regardless of the underlying statistical model being applied.\nThe Morey et al.¬†paper also covers how smaller confidence intervals don‚Äôt necessarily mean more precise estimates, which is another common misinterpretation (with some more discussion here). It‚Äôs true for some intervals.\nAlso, readers may not appreciate the difference between precision and accuracy, and think an estimate is better and assign more trust to it just because it‚Äôs precise, even if it‚Äôs completely inaccurate.\n\nOther gotchas I think make confidence intervals difficult to interpret:\n\nWhat does it mean to be confident anyway? What is a confidence level? Do most people interpret being confident in a confidence interval as meaning the interval is likely to contain the true value?\nThere‚Äôs some real mental gymnastics to remember that a 99% confidence interval is wider than a 95% confidence interval. A 99% interval is wider, so there‚Äôs more uncertainty, but we‚Äôre now 99% confident instead of 95% confident. ü§∑ To me, being more confident would meaning being able to provide a narrower interval with less uncertainty.\nWhen two confidence intervals do not overlap, it means that the difference between the two parameters is statistically significant. But when two confidence intervals do overlap, they may or may not be significantly different. Overlapping intervals are often interpreted as meaning that the difference is not statistically significant, but this is not always the case.\n\nIf I were to include confidence intervals in my writing then I‚Äôd expect the audience to have some level of technical knowledge, and I think it‚Äôs important to include what procedure has been used to compute the interval. If we think it is reasonable to interpret the interval as a Bayesian credible interval, then we can tell the audience that‚Äôs what we‚Äôre doing, and also include information about the prior we‚Äôve used as part of our methodology."
  },
  {
    "objectID": "blog/confidence-intervals-in-the-wild/index.html#more-reading",
    "href": "blog/confidence-intervals-in-the-wild/index.html#more-reading",
    "title": "Confidence intervals in the wild",
    "section": "More reading",
    "text": "More reading\nSome posts from Andrew Gelman‚Äôs blog:\n\nProblematic interpretations of confidence intervals\nAbraham Lincoln and confidence intervals\nHow to interpret confidence intervals?\nConfidence intervals, compatability intervals, uncertainty intervals\n\nTwo papers from Richard Morey et al.\n\nRobust misinterpretation of confidence intervals\nThe fallacy of placing confidence in confidence intervals"
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html",
    "href": "blog/how-long-will-my-machine-last/index.html",
    "title": "How long will my machine last?",
    "section": "",
    "text": "When writing about confidence intervals I came across this 1974 paper by E. T. Jaynes which had a nice problem about estimating the minimum lifetime of some machinery.\nI love this kind of ‚Äúreal life‚Äù problem. The paper makes the point that calculating a confidence interval is complicated and results in a completely unhelpful interval. It shows how a Bayesian approach gives much better results, but the paper jumps through to defining a posterior density without showing how it gets there. I wanted to work it though and thought it would be a chance to try out using Stan."
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#some-maths",
    "href": "blog/how-long-will-my-machine-last/index.html#some-maths",
    "title": "How long will my machine last?",
    "section": "Some maths",
    "text": "Some maths\nPoisson processes can be used to model the number of events which occur over time ‚Äì things like the number of phonecalls received in a call centre, the number of people who walk into a shop, or the number of cars which pass you by as you walk down the street.\nIf we denote the number of events to occur up to some time \\(t\\) as \\(N(t)\\), then the probability that \\(N(t) = n\\) is given by the Poisson distribution:\n\\[ P(N(t) = n) = \\frac{(\\lambda t)^n}{n!} e^{-\\lambda t} \\]\nwhere \\(\\lambda\\) is the rate of events per unit time.\nIn our example an event is the failure of a machine. We‚Äôre interested in the time of each failure which we label as \\(T_1, T_2, \\ldots, T_N\\). Each of these times is independent of the others.\nWe can notice that the event \\(T_1 &gt; t\\) is equivalent to saying that no events occurred by time \\(t\\), which is the same as saying \\(N(t) = 0\\). So we can write\n\\[ P(T_1 &gt; t) = P(N(t) = 0) = e^{-\\lambda t}. \\]\nProbabilities must sum to one, so we can also note\n\\[ P(T_1 \\leq t) = 1 - P(T_1 &gt; t) = 1 - e^{-\\lambda t}. \\]\nThe result, \\(1 - e^{-\\lambda t}\\), is the cumulative distribution function for an exponential distribution with rate \\(lambda\\), with probability density function\n\\[ p(t \\mid \\lambda) = \\lambda e^{-\\lambda t}. \\]\nThis means that \\(T_1 \\sim \\text{Exponential}(\\lambda)\\). By using the memorylessness property of the exponential distribution we can say that all \\(T_1, T_2, T_3\\) etc. are exponentially distributed.\n\nIn our problem there‚Äôs an added constraint ‚Äì the machines will run for a time \\(\\theta\\) before the chemical inhibitor is exhausted and failures begin. This means that the failure times have a lower bound ‚Äì they will be at least \\(\\theta\\) ‚Äì which leads us to describe the failure times as following a shifted exponential distribution\n\\[\np(t \\mid \\lambda, \\theta) =\n\\begin{cases}\n    \\lambda \\exp(-\\lambda (t - \\theta)) & x \\geq \\theta, \\\\\n    0 & x &lt; \\theta.\n\\end{cases}\n\\]\nIf we set \\(\\lambda = 1\\) and multiply out the negative sign, this would be equivalent to the distribution given in the Jaynes paper.\nWe can see the effect of the shifting by plotting the probability density function for different values of \\(\\theta\\):\n\n\nCode\nx_length = 8;\n\nviewof lambda = Inputs.range([0.1, 3], {\n  label: tex`\\text{lambda: }\\lambda`,\n  step: 0.1,\n  value: 1,\n});\nviewof theta = Inputs.range([0, x_length], {\n  label: tex`\\text{theta: }\\theta`,\n  step: 0.1,\n  value: 0,\n});\nviewof t = Inputs.range([0, x_length], {\n  label: tex`\\Pr(X \\leq t): t`,\n  step: 0.1,\n  value: 3,\n});\n\nx = Array.from({ length: x_length * 100 + 1 }, (_, i) =&gt; i / 100);\ny = x.map((x_i) =&gt; lambda * Math.exp(-lambda * (x_i - theta)));\ncoordinates = x.map((x_i, i) =&gt; {\n  return { x: x_i, y: (x_i &gt;= theta) * y[i] };\n});\n\nfilteredCoordinates = coordinates.filter((coord) =&gt; coord.x &lt;= t);\n\nPlot.plot({\n  x: { domain: [0, x_length] },\n  marks: [\n    Plot.lineY(coordinates, { x: \"x\", y: \"y\" }),\n    // Add the stuff for the cumulative probability\n    Plot.areaY(filteredCoordinates, {\n      x: \"x\",\n      y: \"y\",\n      fill: \"blue\",\n      fillOpacity: 0.5,\n    }),\n    Plot.lineY(filteredCoordinates, {\n      x: \"x\",\n      y: \"y\",\n      stroke: \"blue\",\n      strokeWidth: 4,\n    }),\n    Plot.ruleX([[t]]),\n  ],\n});"
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#modelling-as-a-bayesian-problem",
    "href": "blog/how-long-will-my-machine-last/index.html#modelling-as-a-bayesian-problem",
    "title": "How long will my machine last?",
    "section": "Modelling as a Bayesian problem",
    "text": "Modelling as a Bayesian problem\nThe Jaynes paper derives a 90% credible interval for \\(\\theta\\). A 90% credible interval is a range of values where there‚Äôs a 90% chance that the actual value is within that range (a property which is not shared by confidence intervals!)\nOur goal is to get a formula for the probability density of \\(\\theta\\) by using the observed failure times, so we can derive an interval for \\(\\theta\\).\nThe Jaynes paper derives the posterior probability density for \\(\\theta\\) as\n\n\\[\np(\\theta \\mid x_1, x_2, \\ldots x_N) = \\begin{cases}\n  N\\exp(N(\\theta - \\min(t))), & \\theta &lt; \\min(t), \\\\\n  0, & \\theta &gt; \\min(t).\n\\end{cases}\n\\]\n\nWe can derive this using Bayes‚Äô theorem,\n\\[ p(\\theta \\mid t_1, t_2, \\ldots t_N) = \\frac{p(t_1, t_2, \\ldots t_N \\mid \\theta)p(\\theta)}{p(t_1, t_2, \\ldots t_N)}. \\]\nWe choose a flat prior for \\(\\theta\\), such that \\(p(\\theta) \\propto 1\\) and set \\(\\lambda = 1\\) throughout. Since \\(p(\\theta) \\propto 1\\) and \\(p(t_1, t_2, \\ldots t_N)\\) doesn‚Äôt depend on \\(\\theta\\), we drop them as constants, ending up with\n\\[\np(\\theta \\mid t_1, t_2, \\ldots t_N) \\propto p(t_1, t_2, \\ldots t_N \\mid \\theta).\n\\]\nUsing the independence of each failure time, we continue to simplify the expression:\n\\[\n\\begin{align}\n    p(\\theta \\mid t_1, t_2, \\ldots t_N) & \\propto p(t_1, t_2, \\ldots t_N \\mid \\theta) \\\\\n    &\\propto \\prod_{i=1}^{N} p(t_i \\mid \\theta) \\\\\n    &\\propto \\prod_{i=1}^{N} \\exp{(\\theta - t_i)} \\cdot \\mathbb{1} \\{ t_i \\geq \\theta \\}\n\\end{align}\n\\]\nwhere \\(\\mathbb{1} \\{ t_i \\geq \\theta \\}\\) is the indicator function which is one if \\(t_i \\geq \\theta\\) and zero otherwise. Continuing, we note that the union of the events \\(\\{ t_i \\geq \\theta \\}\\) is the same as the event \\(\\min(t) \\geq \\theta\\):\n\\[\n\\begin{align}\n    p(\\theta \\mid t_1, t_2, \\ldots t_N) &\\propto\n        \\prod_{i=1}^{N} \\exp{(\\theta - t_i)} \\cdot \\mathbb{1} \\{ t_i \\geq \\theta \\} \\\\\n    & \\propto e^{N \\theta} \\cdot \\mathbb{1} \\{ \\min(t) \\geq \\theta \\}.\n\\end{align}\n\\]\nTo get a probability distribution for \\(\\theta\\), we recall that probability distributions must integrate to one,\n\\[ \\int_{-\\infty}^{\\infty} p(\\theta \\mid t_1, t_2, \\ldots t_N) d\\theta = 1. \\]\nWe look to integrate our formula for \\(p(\\theta \\mid t_1, t_2, \\ldots t_N)\\). We can use the result to form a probability density which integrates to one,\n\\[\n\\begin{align}\n\\int_{-\\infty}^{\\infty} p(\\theta \\mid t_1, t_2, \\ldots t_N) d\\theta\n    &= \\int_{-\\infty}^{\\infty} e^{N \\theta} \\cdot \\mathbb{1} \\{\\min(t) \\geq \\theta \\} d\\theta \\\\\n    &= \\int_{-\\infty}^{\\min(t)} e^{N \\theta} d\\theta \\\\\n    &= \\frac{\\exp{(N\\min(t))}}{N} .\n\\end{align}\n\\]\nIf we divide our formula \\(e^{N \\theta} \\cdot \\mathbb{1} \\{\\min(t) \\geq \\theta \\}\\) by \\(\\frac{\\exp{(N\\min(t))}}{N}\\) we get a probability distribution for \\(\\theta\\) which integrates to one,\n\\[\np(\\theta \\mid t_1, t_2, \\ldots t_N) =\n\\begin{cases}\n    N\\exp(N(\\theta - \\min(t))) & \\text{if } \\theta \\leq \\min(t), \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\nwhich is the posterior probability given in the paper."
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#deriving-a-credible-interval",
    "href": "blog/how-long-will-my-machine-last/index.html#deriving-a-credible-interval",
    "title": "How long will my machine last?",
    "section": "Deriving a credible interval",
    "text": "Deriving a credible interval\nThe paper provides three observed values for failure times: \\({t_1, t_2, t_3} = {12, 14, 16}\\). From this it derives a 90% credible interval for \\(\\theta\\) of \\(11.23 &lt; \\theta &lt; 12\\); this is what we aim to reproduce.\nThe resulting formula we‚Äôre trying to solve is to find \\(L\\) and \\(U\\) such that\n\\[ \\int_L^U p(\\theta \\mid t_1, t_2, t_3) = 0.9. \\]\nWe know that \\(\\theta \\leq \\min(t)\\) and want to find the shortest credible interval, which means targeting the interval over the area with the bulk of the probability density. Since most of the probability density is near \\(\\min(t)\\), we set \\(U = \\min(t) = 12\\). Substituting in everything else, we can solve for \\(L\\):\n\\[ \\int_L^{12} 3\\exp(3(\\theta - 12)) = 0.9. \\]\nLet‚Äôs expand the integral:\n\\[\n\\begin{align}\n    \\int_L^{12} 3\\exp(3(\\theta - 12)) &= \\exp(3(\\theta - 12)) \\big|_L^{12} \\\\\n    &= \\vphantom{\\big|_L^{12}} \\exp(0) - \\exp(3(L - 12)) \\\\\n    &= \\vphantom{\\big|_L^{12}} 1 - \\exp(3(L - 12)).\n\\end{align}\n\\]\nFinally, we solve for \\(L\\):\n\\[ \\begin{align} 1 - \\exp(3(L - 12)) &= 0.9 \\\\ &\\implies L = 11.2325, \\end{align} \\]\nand with that, the interval we‚Äôve computed is the same as Jaynes‚Äô:\n\\[ 11.23 &lt; \\theta &lt; 12. \\]"
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#simulations-with-stan",
    "href": "blog/how-long-will-my-machine-last/index.html#simulations-with-stan",
    "title": "How long will my machine last?",
    "section": "Simulations with Stan",
    "text": "Simulations with Stan\nSometimes, deriving a posterior probability density is hard, and it‚Äôs not always possible to do so analytically (i.e.¬†to get a nice formula we can work with). In these cases, we can use a simulation approach to get a posterior probability density. Stan is a probabilistic programming language which allows us to do this by performing Markov chain Monte Carlo (MCMC) sampling.\nWe write a Stan program to simulate the posterior probability density for \\(\\theta\\) given the observed values for failure times. Stan will simulate many possible values for \\(\\theta\\), and we can use these to approximate the posterior probability density.\n\ndata {\n    int&lt;lower=0&gt; N;  // Number of observations, a non-negative integer.\n    real&lt;lower=0&gt; t[N];  // Array of N observations, which are real numbers.\n}\n\nparameters {\n    // The parameter theta, constrained to be between 0 and the minimum value\n    // of t.\n    real&lt;lower=0, upper=min(t)&gt; theta; \n}\n\nmodel {\n    // We don't explicitly write a prior for theta. Stan will assume theta\n    // has a flat prior on (0, min(t)).\n\n    // We specify a prior for each observation. Stan has a helpful syntax\n    // for specifying truncated distributions.\n    for (n in 1:N) {\n        t[n] ~ exponential(1) T[theta, ];\n    }\n}\n\nWe can break this syntax down by its blocks:\n\nThe data block is used to declare the data that will be input to the model. In this case, there are two data inputs: N, an integer representing the number of observations, and t, an array of real numbers representing the observations themselves. We will provide these values to the program when we run it.\nThe parameters block is used to declare parameters used in the statistical model, which do not need to be provided by the user. In this case, there is one parameter, theta, which we are estimating, and we specify it is a real number constrained to be between 0 and the minimum value of t.\nThe model block is used to specify the statistical model ‚Äì this is typically done by specifying the probability distribution of the data and the relationships to the defined parameters. In this case we specify a prior density for each observation t[n]. We use the exponential distribution, and set \\(\\lambda = 1\\). We also specify that the distribution is truncated at \\(\\theta\\), using the T[theta, ] syntax, which is how we achieve specifying the shifted exponential distribution.\n\nWe can run the model using the rstan package in R. We provide the data inputs N and t and run the model for 20,000 iterations, using 4 chains.\n\nlibrary(rstan)\nlibrary(dplyr)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nset.seed(123)\n\nN &lt;- 3\nt &lt;- c(12, 14, 16)\n\nfit &lt;- sampling(model, list(N = N, t = t), iter = 20000, chains = 4)\n\nprint(fit)\n#&gt; Inference for Stan model: anon_model.\n#&gt; 4 chains, each with iter=20000; warmup=10000; thin=1; \n#&gt; post-warmup draws per chain=10000, total post-warmup draws=40000.\n#&gt; \n#&gt;        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n#&gt; theta 11.67    0.00 0.34 10.78 11.55 11.78 11.91 11.99 15295    1\n#&gt; lp__  33.29    0.01 0.81 30.98 33.08 33.60 33.81 33.87  9341    1\n#&gt; \n#&gt; Samples were drawn using NUTS(diag_e) at Wed May 29 21:47:55 2024.\n#&gt; For each parameter, n_eff is a crude measure of effective sample size,\n#&gt; and Rhat is the potential scale reduction factor on split chains (at \n#&gt; convergence, Rhat=1).\n\nThe model gives a nice summary of the results, including the mean, standard deviation, and quantiles for each parameter. We can see here that the model has provided a central estimate of \\(\\theta\\) to be 11.67.\nWe can use R to access the samples from the model and plot the posterior probability density for \\(\\theta\\) using a histogram.\n\nparams &lt;- extract(fit)\nsamples &lt;- tibble::as_tibble(params)\n\nhist(samples[[\"theta\"]])"
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#conclusion",
    "href": "blog/how-long-will-my-machine-last/index.html#conclusion",
    "title": "How long will my machine last?",
    "section": "Conclusion",
    "text": "Conclusion\nWe can use the samples generated from our Monte Carlo simulation to estimate the probability that \\(\\theta\\) is between 11.23 and 12:\n\nsamples |&gt;\n    select(theta) |&gt;\n    count(11.23 &lt; theta & theta &lt; 12) |&gt;\n    mutate(prop = n / sum(n))\n#&gt; # A tibble: 2 √ó 3\n#&gt;   `11.23 &lt; theta & theta &lt; 12`     n   prop\n#&gt;   &lt;lgl&gt;                        &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1 FALSE                         3969 0.0992\n#&gt; 2 TRUE                         36031 0.901\n\nThe results tell us that 90.1% of the simulated samples of \\(\\theta\\) are in this range, which shows that 11.12 and 12 is a 90% credible interval for \\(\\theta\\). We‚Äôve managed to reproduce the result from the Jaynes paper!"
  }
]