[
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html",
    "href": "blog/how-long-will-my-machine-last/index.html",
    "title": "How long will my machine last?",
    "section": "",
    "text": "When writing about confidence intervals I came across this 1974 paper by E. T. Jaynes which had a nice problem about estimating the minimum lifetime of some machinery.\nI love this kind of “real life” problem. The paper makes the point that calculating a confidence interval is complicated and results in a completely unhelpful interval. It shows how a Bayesian approach gives much better results, but the paper jumps through to defining a posterior density without showing how it gets there. I wanted to work it though and thought it would be a chance to try out using Stan."
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#some-maths",
    "href": "blog/how-long-will-my-machine-last/index.html#some-maths",
    "title": "How long will my machine last?",
    "section": "Some maths",
    "text": "Some maths\nPoisson processes can be used to model the number of events which occur over time – things like the number of phonecalls received in a call centre, the number of people who walk into a shop, or the number of cars which pass you by as you walk down the street.\nIf we denote the number of events to occur up to some time \\(t\\) as \\(N(t)\\), then the probability that \\(N(t) = n\\) is given by the Poisson distribution:\n\\[ P(N(t) = n) = \\frac{(\\lambda t)^n}{n!} e^{-\\lambda t} \\]\nwhere \\(\\lambda\\) is the rate of events per unit time.\nIn our example an event is the failure of a machine. We’re interested in the time of each failure which we label as \\(T_1, T_2, \\ldots, T_N\\). Each of these times is independent of the others.\nWe can notice that the event \\(T_1 &gt; t\\) is equivalent to saying that no events occurred by time \\(t\\), which is the same as saying \\(N(t) = 0\\). So we can write\n\\[ P(T_1 &gt; t) = P(N(t) = 0) = e^{-\\lambda t}. \\]\nProbabilities must sum to one, so we can also note\n\\[ P(T_1 \\leq t) = 1 - P(T_1 &gt; t) = 1 - e^{-\\lambda t}. \\]\nThe result, \\(1 - e^{-\\lambda t}\\), is the cumulative distribution function for an exponential distribution with rate \\(lambda\\), with probability density function\n\\[ p(t \\mid \\lambda) = \\lambda e^{-\\lambda t}. \\]\nThis means that \\(T_1 \\sim \\text{Exponential}(\\lambda)\\). By using the memorylessness property of the exponential distribution we can say that all \\(T_1, T_2, T_3\\) etc. are exponentially distributed.\n\nIn our problem there’s an added constraint – the machines will run for a time \\(\\theta\\) before the chemical inhibitor is exhausted and failures begin. This means that the failure times have a lower bound – they will be at least \\(\\theta\\) – which leads us to describe the failure times as following a shifted exponential distribution\n\\[\np(t \\mid \\lambda, \\theta) =\n\\begin{cases}\n    \\lambda \\exp(-\\lambda (t - \\theta)) & x \\geq \\theta, \\\\\n    0 & x &lt; \\theta.\n\\end{cases}\n\\]\nIf we set \\(\\lambda = 1\\) and multiply out the negative sign, this would be equivalent to the distribution given in the Jaynes paper.\nWe can see the effect of the shifting by plotting the probability density function for different values of \\(\\theta\\):\n\n\nCode\nx_length = 8;\n\nviewof lambda = Inputs.range([0.1, 3], {\n  label: tex`\\text{lambda: }\\lambda`,\n  step: 0.1,\n  value: 1,\n});\nviewof theta = Inputs.range([0, x_length], {\n  label: tex`\\text{theta: }\\theta`,\n  step: 0.1,\n  value: 0,\n});\nviewof t = Inputs.range([0, x_length], {\n  label: tex`\\Pr(X \\leq t): t`,\n  step: 0.1,\n  value: 3,\n});\n\nx = Array.from({ length: x_length * 100 + 1 }, (_, i) =&gt; i / 100);\ny = x.map((x_i) =&gt; lambda * Math.exp(-lambda * (x_i - theta)));\ncoordinates = x.map((x_i, i) =&gt; {\n  return { x: x_i, y: (x_i &gt;= theta) * y[i] };\n});\n\nfilteredCoordinates = coordinates.filter((coord) =&gt; coord.x &lt;= t);\n\nPlot.plot({\n  x: { domain: [0, x_length] },\n  marks: [\n    Plot.lineY(coordinates, { x: \"x\", y: \"y\" }),\n    // Add the stuff for the cumulative probability\n    Plot.areaY(filteredCoordinates, {\n      x: \"x\",\n      y: \"y\",\n      fill: \"blue\",\n      fillOpacity: 0.5,\n    }),\n    Plot.lineY(filteredCoordinates, {\n      x: \"x\",\n      y: \"y\",\n      stroke: \"blue\",\n      strokeWidth: 4,\n    }),\n    Plot.ruleX([[t]]),\n  ],\n});"
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#modelling-as-a-bayesian-problem",
    "href": "blog/how-long-will-my-machine-last/index.html#modelling-as-a-bayesian-problem",
    "title": "How long will my machine last?",
    "section": "Modelling as a Bayesian problem",
    "text": "Modelling as a Bayesian problem\nThe Jaynes paper derives a 90% credible interval for \\(\\theta\\). A 90% credible interval is a range of values where there’s a 90% chance that the actual value is within that range (a property which is not shared by confidence intervals!)\nOur goal is to get a formula for the probability density of \\(\\theta\\) by using the observed failure times, so we can derive an interval for \\(\\theta\\).\nThe Jaynes paper derives the posterior probability density for \\(\\theta\\) as\n\n\\[\np(\\theta \\mid x_1, x_2, \\ldots x_N) = \\begin{cases}\n  N\\exp(N(\\theta - \\min(t))), & \\theta &lt; \\min(t), \\\\\n  0, & \\theta &gt; \\min(t).\n\\end{cases}\n\\]\n\nWe can derive this using Bayes’ theorem,\n\\[ p(\\theta \\mid t_1, t_2, \\ldots t_N) = \\frac{p(t_1, t_2, \\ldots t_N \\mid \\theta)p(\\theta)}{p(t_1, t_2, \\ldots t_N)}. \\]\nWe choose a flat prior for \\(\\theta\\), such that \\(p(\\theta) \\propto 1\\) and set \\(\\lambda = 1\\) throughout. Since \\(p(\\theta) \\propto 1\\) and \\(p(t_1, t_2, \\ldots t_N)\\) doesn’t depend on \\(\\theta\\), we drop them as constants, ending up with\n\\[\np(\\theta \\mid t_1, t_2, \\ldots t_N) \\propto p(t_1, t_2, \\ldots t_N \\mid \\theta).\n\\]\nUsing the independence of each failure time, we continue to simplify the expression:\n\\[\n\\begin{align}\n    p(\\theta \\mid t_1, t_2, \\ldots t_N) & \\propto p(t_1, t_2, \\ldots t_N \\mid \\theta) \\\\\n    &\\propto \\prod_{i=1}^{N} p(t_i \\mid \\theta) \\\\\n    &\\propto \\prod_{i=1}^{N} \\exp{(\\theta - t_i)} \\cdot \\mathbb{1} \\{ t_i \\geq \\theta \\}\n\\end{align}\n\\]\nwhere \\(\\mathbb{1} \\{ t_i \\geq \\theta \\}\\) is the indicator function which is one if \\(t_i \\geq \\theta\\) and zero otherwise. Continuing, we note that the union of the events \\(\\{ t_i \\geq \\theta \\}\\) is the same as the event \\(\\min(t) \\geq \\theta\\):\n\\[\n\\begin{align}\n    p(\\theta \\mid t_1, t_2, \\ldots t_N) &\\propto\n        \\prod_{i=1}^{N} \\exp{(\\theta - t_i)} \\cdot \\mathbb{1} \\{ t_i \\geq \\theta \\} \\\\\n    & \\propto e^{N \\theta} \\cdot \\mathbb{1} \\{ \\min(t) \\geq \\theta \\}.\n\\end{align}\n\\]\nTo get a probability distribution for \\(\\theta\\), we recall that probability distributions must integrate to one,\n\\[ \\int_{-\\infty}^{\\infty} p(\\theta \\mid t_1, t_2, \\ldots t_N) d\\theta = 1. \\]\nWe look to integrate our formula for \\(p(\\theta \\mid t_1, t_2, \\ldots t_N)\\). We can use the result to form a probability density which integrates to one,\n\\[\n\\begin{align}\n\\int_{-\\infty}^{\\infty} p(\\theta \\mid t_1, t_2, \\ldots t_N) d\\theta\n    &= \\int_{-\\infty}^{\\infty} e^{N \\theta} \\cdot \\mathbb{1} \\{\\min(t) \\geq \\theta \\} d\\theta \\\\\n    &= \\int_{-\\infty}^{\\min(t)} e^{N \\theta} d\\theta \\\\\n    &= \\frac{\\exp{(N\\min(t))}}{N} .\n\\end{align}\n\\]\nIf we divide our formula \\(e^{N \\theta} \\cdot \\mathbb{1} \\{\\min(t) \\geq \\theta \\}\\) by \\(\\frac{\\exp{(N\\min(t))}}{N}\\) we get a probability distribution for \\(\\theta\\) which integrates to one,\n\\[\np(\\theta \\mid t_1, t_2, \\ldots t_N) =\n\\begin{cases}\n    N\\exp(N(\\theta - \\min(t))) & \\text{if } \\theta \\leq \\min(t), \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\nwhich is the posterior probability given in the paper."
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#deriving-a-credible-interval",
    "href": "blog/how-long-will-my-machine-last/index.html#deriving-a-credible-interval",
    "title": "How long will my machine last?",
    "section": "Deriving a credible interval",
    "text": "Deriving a credible interval\nThe paper provides three observed values for failure times: \\({t_1, t_2, t_3} = {12, 14, 16}\\). From this it derives a 90% credible interval for \\(\\theta\\) of \\(11.23 &lt; \\theta &lt; 12\\); this is what we aim to reproduce.\nThe resulting formula we’re trying to solve is to find \\(L\\) and \\(U\\) such that\n\\[ \\int_L^U p(\\theta \\mid t_1, t_2, t_3) = 0.9. \\]\nWe know that \\(\\theta \\leq \\min(t)\\) and want to find the shortest credible interval, which means targeting the interval over the area with the bulk of the probability density. Since most of the probability density is near \\(\\min(t)\\), we set \\(U = \\min(t) = 12\\). Substituting in everything else, we can solve for \\(L\\):\n\\[ \\int_L^{12} 3\\exp(3(\\theta - 12)) = 0.9. \\]\nLet’s expand the integral:\n\\[\n\\begin{align}\n    \\int_L^{12} 3\\exp(3(\\theta - 12)) &= \\exp(3(\\theta - 12)) \\big|_L^{12} \\\\\n    &= \\vphantom{\\big|_L^{12}} \\exp(0) - \\exp(3(L - 12)) \\\\\n    &= \\vphantom{\\big|_L^{12}} 1 - \\exp(3(L - 12)).\n\\end{align}\n\\]\nFinally, we solve for \\(L\\):\n\\[ \\begin{align} 1 - \\exp(3(L - 12)) &= 0.9 \\\\ &\\implies L = 11.2325, \\end{align} \\]\nand with that, the interval we’ve computed is the same as Jaynes’:\n\\[ 11.23 &lt; \\theta &lt; 12. \\]"
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#simulations-with-stan",
    "href": "blog/how-long-will-my-machine-last/index.html#simulations-with-stan",
    "title": "How long will my machine last?",
    "section": "Simulations with Stan",
    "text": "Simulations with Stan\nSometimes, deriving a posterior probability density is hard, and it’s not always possible to do so analytically (i.e. to get a nice formula we can work with). In these cases, we can use a simulation approach to get a posterior probability density. Stan is a probabilistic programming language which allows us to do this by performing Markov chain Monte Carlo (MCMC) sampling.\nWe write a Stan program to simulate the posterior probability density for \\(\\theta\\) given the observed values for failure times. Stan will simulate many possible values for \\(\\theta\\), and we can use these to approximate the posterior probability density.\n\ndata {\n    int&lt;lower=0&gt; N;  // Number of observations, a non-negative integer.\n    real&lt;lower=0&gt; t[N];  // Array of N observations, which are real numbers.\n}\n\nparameters {\n    // The parameter theta, constrained to be between 0 and the minimum value\n    // of t.\n    real&lt;lower=0, upper=min(t)&gt; theta; \n}\n\nmodel {\n    // We don't explicitly write a prior for theta. Stan will assume theta\n    // has a flat prior on (0, min(t)).\n\n    // We specify a prior for each observation. Stan has a helpful syntax\n    // for specifying truncated distributions.\n    for (n in 1:N) {\n        t[n] ~ exponential(1) T[theta, ];\n    }\n}\n\nWe can break this syntax down by its blocks:\n\nThe data block is used to declare the data that will be input to the model. In this case, there are two data inputs: N, an integer representing the number of observations, and t, an array of real numbers representing the observations themselves. We will provide these values to the program when we run it.\nThe parameters block is used to declare parameters used in the statistical model, which do not need to be provided by the user. In this case, there is one parameter, theta, which we are estimating, and we specify it is a real number constrained to be between 0 and the minimum value of t.\nThe model block is used to specify the statistical model – this is typically done by specifying the probability distribution of the data and the relationships to the defined parameters. In this case we specify a prior density for each observation t[n]. We use the exponential distribution, and set \\(\\lambda = 1\\). We also specify that the distribution is truncated at \\(\\theta\\), using the T[theta, ] syntax, which is how we achieve specifying the shifted exponential distribution.\n\nWe can run the model using the rstan package in R. We provide the data inputs N and t and run the model for 20,000 iterations, using 4 chains.\n\nlibrary(rstan)\nlibrary(dplyr)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nset.seed(123)\n\nN &lt;- 3\nt &lt;- c(12, 14, 16)\n\nfit &lt;- sampling(model, list(N = N, t = t), iter = 20000, chains = 4)\n\nprint(fit)\n\nThe model gives a nice summary of the results, including the mean, standard deviation, and quantiles for each parameter. We can see here that the model has provided a central estimate of \\(\\theta\\) to be 11.67.\nWe can use R to access the samples from the model and plot the posterior probability density for \\(\\theta\\) using a histogram.\n\nparams &lt;- extract(fit)\nsamples &lt;- tibble::as_tibble(params)\n\nhist(samples[[\"theta\"]])"
  },
  {
    "objectID": "blog/how-long-will-my-machine-last/index.html#conclusion",
    "href": "blog/how-long-will-my-machine-last/index.html#conclusion",
    "title": "How long will my machine last?",
    "section": "Conclusion",
    "text": "Conclusion\nWe can use the samples generated from our Monte Carlo simulation to estimate the probability that \\(\\theta\\) is between 11.23 and 12:\n\nsamples |&gt;\n    select(theta) |&gt;\n    count(11.23 &lt; theta & theta &lt; 12) |&gt;\n    mutate(prop = n / sum(n))\n#&gt; # A tibble: 2 × 3\n#&gt;   `11.23 &lt; theta & theta &lt; 12`     n  prop\n#&gt;   &lt;lgl[1d]&gt;                    &lt;int&gt; &lt;dbl&gt;\n#&gt; 1 FALSE                         4039 0.101\n#&gt; 2 TRUE                         35961 0.899\n\nThe results tell us that 89.9% of the simulated samples of \\(\\theta\\) are in this range, which shows that 11.12 and 12 is a 90% credible interval for \\(\\theta\\). We’ve managed to reproduce the result from the Jaynes paper!"
  },
  {
    "objectID": "blog/confidence-intervals-in-the-wild/index.html",
    "href": "blog/confidence-intervals-in-the-wild/index.html",
    "title": "Confidence intervals in the wild",
    "section": "",
    "text": "Confidence intervals have always come up during interviews for analytical roles. I’m usually asked what a confidence interval is, and I’ll respond along the lines of the definition given by the Office for National Statistics.\nThis definition aligns with that I was taught – if we took many samples and created many 95% confidence intervals, we’d expect 95% of those intervals to contain the true value for the parameter of interest. For many X% confidence intervals, we’d expect X% of the intervals to contain the true value.\nI’ve then been asked how I would help a non-technical audience interpret a specific confidence interval. The thing is, I’ve never felt that confidence intervals are a good thing to use with that kind of audience. Many statisticians misinterpret them, so how we could expect a non-technical audience to use them correctly?\nI’ve gotten into some awkward debates over this – so here’s me getting my thoughts together."
  },
  {
    "objectID": "blog/confidence-intervals-in-the-wild/index.html#what-does-a-confidence-interval-actually-tell-you",
    "href": "blog/confidence-intervals-in-the-wild/index.html#what-does-a-confidence-interval-actually-tell-you",
    "title": "Confidence intervals in the wild",
    "section": "What does a confidence interval actually tell you?",
    "text": "What does a confidence interval actually tell you?\nI’d see a confidence interval in the wild and feel a bit unsure of how I could interpret it. If the definition of a confidence interval describes a behavior across many intervals, what does it mean when we’re presented with a single interval?\nA quick google and we see some common interpretations of confidence intervals:\n\nThere’s a 95% chance that the parameter of interest lies within a 95% confidence interval.\nConfidence intervals are a range in which we think the true value is likely to lie.\n\nI couldn’t see how either of these followed from the definition we used earlier, about creating many intervals and on average X% of them containing the true value. I went searching and found many blog posts on the subject from Andrew Gelman, one of which introduced this paper from Richard Morey et al. which has shaped my thoughts over the years.\n\nTake this example from some of the ONS’s COVID-19 reporting:\n\nThe latest estimated rate of coronavirus (COVID-19) reinfections on 28 October 2022 was 42.8 per 100,000 participant days at risk (95% confidence interval: 42.0 to 43.6).\n\nHere we have a single 95% interval reported alongside an estimate of the COVID-19 reinfection rate. Maybe this is one of the lucky intervals which contains the true reinfection rate, but maybe it’s not. We don’t know.\nThe idea behind the 95% confidence interval is to perform some procedure which, on average, returns intervals \\((L, U)\\) which contain the true parameter 95% of the time.\n\\[P(L(X) \\leq \\theta \\leq U(X)) = 0.95\\]\nWe write \\(L(X)\\) and \\(U(X)\\) to emphasise these are random variables which in practice will often depend on the data. The problem occurs when we try to replace \\((L(X), U(X))\\) with an actual interval we’ve obtained.\nIf \\(\\theta\\) was the true COVID-19 reinfection rate, \\(\\theta\\) is just some fixed number we don’t know the value of. We can’t say anything probabilistic about the value of \\(\\theta\\) (unless we start using Bayesian techniques).\nBut, if we replace \\(L(X)\\) and \\(U(X)\\) above with the obtained interval, the probability changes.\n\\[\n  P(42.0 \\leq \\theta \\leq 43.6) = \\begin{cases}\n    1, & \\text{if } 42.0 \\leq \\theta \\leq 43.6, \\\\\n    0, & \\text{otherwise}.\n  \\end{cases}\n\\]\nAndrew Gelman provides an analogy in a blog post to explain why the probability changes once you substitute actual values into the interval. To provide a similar analogy – let’s say 170cm is a fairly average male height – we could imagine that half of men are taller and half are shorter than 170cm. Bob is 168cm tall.\nIt seems reasonable to say, if \\(X\\) was the height of some arbitrary man, that the probability of that man being taller than 170cm is a half.\n\\[P(X &gt; 170\\text{cm}) = 0.5.\\]\nBut we can’t just replace \\(X\\), which represents the height of some arbitrary man. with Bob’s height and expect the probability to remain the same. If we condition the above probability to talk specifically about Bob, rather than some arbitrary man, then the probability becomes 0, as Bob is shorter than 170cm.\n\\[P(X &gt; 170\\text{cm} \\mid X = \\text{Bob's height}) = P(168\\text{cm} &gt; 170\\text{cm}) = 0.\\]\nJerzy Neyman (who introduced confidence intervals in this 1937 paper) was so clear about their interpretation:\n\nIt will be noticed that in the above description the probability statements refer to the problems of estimation with which the statistician will be concerned in the future. In fact, I have repeatedly stated that the frequency of correct results will tend to \\(\\alpha\\). Consider now the case when a sample \\(E'\\), is already drawn, and the calculations have given \\(\\underset{\\bar{}}{\\theta}(E') = 1\\) and \\(\\bar{\\theta}(E') = 2\\). Can we say that in this particular case the probability of the true value falling between 1 and 2 is equal to \\(\\alpha\\)?\nThe answer is obviously in the negative. The parameter \\(\\theta_1\\) is an unknown constant, and no probability statement concerning its value may be made, that is except for hypothetical and trivial ones\n\\[\n\\begin{equation} \\tag{21}\n  P(1 \\leq \\theta_1 \\leq 2) = \\begin{cases}\n    1, & \\text{if } 1 \\leq \\theta_1 \\leq 2, \\\\\n    0, & \\text{if either } \\theta_1 \\leq 2 \\text{ or } 2 \\geq \\theta_1.\n  \\end{cases}\n\\end{equation}\n\\]\n\n\nTo sum up – we can’t say that all confidence intervals give a range of likely values for some parameter, at least not from the definition of a confidence interval alone. Some confidence intervals may be sensible to interpret as a Bayesian credible intervals, at which point we could make statements about likely values.\nOr as Morey et al. put it:\n\nIn the case where data are normally distributed, for instance, there is a particular prior that will lead to a confidence interval that is numerically identical to Bayesian credible intervals computed using the Bayesian posterior (Jeffreys, 1961; Lindley, 1965). This might lead one to suspect that it does not matter whether one uses confidence procedures or Bayesian procedures. We showed, however, that confidence intervals and credible intervals can disagree markedly. The only way to know that a confidence interval is numerically identical to some credible interval is to prove it. The correspondence cannot – and should not – be assumed."
  },
  {
    "objectID": "blog/confidence-intervals-in-the-wild/index.html#closing-thoughts",
    "href": "blog/confidence-intervals-in-the-wild/index.html#closing-thoughts",
    "title": "Confidence intervals in the wild",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nI think that confidence intervals have a use, particularly when the procedures to create them provide intervals which help describe the uncertainty in an estimate. But I’d be nervous to include them when writing for non-technical audiences, mainly due to the risk of misinterpretation:\n\nA specific 95% confidence interval doesn’t cover the true value with 95% probability, but many readers will see an interval and assume that.\nA confidence interval doesn’t give a range of likely values for some parameter, unless it is appropriate to interpret it as a Bayesian credible interval – which a non-technical audience wouldn’t be familiar with. A reader may assume all intervals provide likely values, regardless of the underlying statistical model being applied.\nThe Morey et al. paper also covers how smaller confidence intervals don’t necessarily mean more precise estimates, which is another common misinterpretation (with some more discussion here). It’s true for some intervals.\nAlso, readers may not appreciate the difference between precision and accuracy, and think an estimate is better and assign more trust to it just because it’s precise, even if it’s completely inaccurate.\n\nOther gotchas I think make confidence intervals difficult to interpret:\n\nWhat does it mean to be confident anyway? What is a confidence level? Do most people interpret being confident in a confidence interval as meaning the interval is likely to contain the true value?\nThere’s some real mental gymnastics to remember that a 99% confidence interval is wider than a 95% confidence interval. A 99% interval is wider, so there’s more uncertainty, but we’re now 99% confident instead of 95% confident. 🤷 To me, being more confident would meaning being able to provide a narrower interval with less uncertainty.\nWhen two confidence intervals do not overlap, it means that the difference between the two parameters is statistically significant. But when two confidence intervals do overlap, they may or may not be significantly different. Overlapping intervals are often interpreted as meaning that the difference is not statistically significant, but this is not always the case.\n\nIf I were to include confidence intervals in my writing then I’d expect the audience to have some level of technical knowledge, and I think it’s important to include what procedure has been used to compute the interval. If we think it is reasonable to interpret the interval as a Bayesian credible interval, then we can tell the audience that’s what we’re doing, and also include information about the prior we’ve used as part of our methodology."
  },
  {
    "objectID": "blog/confidence-intervals-in-the-wild/index.html#more-reading",
    "href": "blog/confidence-intervals-in-the-wild/index.html#more-reading",
    "title": "Confidence intervals in the wild",
    "section": "More reading",
    "text": "More reading\nSome posts from Andrew Gelman’s blog:\n\nProblematic interpretations of confidence intervals\nAbraham Lincoln and confidence intervals\nHow to interpret confidence intervals?\nConfidence intervals, compatability intervals, uncertainty intervals\n\nTwo papers from Richard Morey et al.\n\nRobust misinterpretation of confidence intervals\nThe fallacy of placing confidence in confidence intervals"
  },
  {
    "objectID": "blog/my-ideal-data-api/index.html",
    "href": "blog/my-ideal-data-api/index.html",
    "title": "My ideal open data API",
    "section": "",
    "text": "Most data across government are published in presentational spreadsheets. These spreadsheets are great for being read by humans (though there’s always ways to make them more accessible), but they’re often not ideal for reusing the data programmatically in tools like R or Python.\nIt’s been noted for a while that having data in a “gazillion spreadsheets” isn’t so helpful, and new services like the UKSHA dashboard, Explore local statistics and planning.data.gov.uk have popped up to provide a better way. These new services haven’t gone unnoticed, and there is demand for services which are built in the open and make data available through simple to understand APIs.\nThe Integrated Data Service aims to bring ready-to-use data to enable faster and wider collaborative analysis for the public good, but looks primarily focussed on making microdata available in a highly secure trusted research environment. Some researchers feel uncertain about the purpose of the service. I feel that there’s huge value to be unlocked in also giving attention to data which are routinely published in difficult to reuse spreadsheets and making that data available, in ready-to-use formats, openly.\nSwitzerland has made cool progress to providing their statistical data as a knowledge graph, and I previously worked on a project looking to achieve a similar thing. I learned a lot in that time and wanted to share the blueprint for what I think would make a pretty good data API for datasets (before I forget it all!)"
  },
  {
    "objectID": "blog/my-ideal-data-api/index.html#goals",
    "href": "blog/my-ideal-data-api/index.html#goals",
    "title": "My ideal open data API",
    "section": "Goals",
    "text": "Goals\nI wouldn’t apply my thinking to every API out there. But I’m thinking specifically about how an analytical customer might get a better experience when trying to work with data.\nI’ve met analysts who aren’t all that familiar with APIs or familiar with JSON, and may not have the tools or experience to properly interact with an API. I was one of them!\nWhen I was getting started I knew that APIs were a way for me to get data for me to analyse by using some URLs. Chaining together multiple API calls, using strange looking URLs, scouring through documentation and being uncertain about whether my code was the problem or whether I’d made a mistake in forming the URL was all a part of the initially painful experience. I was used to working with tabular data, so navigating highly nested JSON in my tools of choice felt disorienting.\nI didn’t know about REST or HATEOAS, or even really care – I just wanted some data!\nWith that in mind, I felt that the goal of an open data API is a design which compliments how an analyst finds and explores data.\n\nWhen I find a dataset on a website like data.gov.uk or ons.gov.uk, I likely arrived there through a Google search. The process of moving from that webpage to importing the data into my statistical software should be straightforward. I want to quickly load the data into my tool to assess whether it’s suitable for my needs.\nImagine the specific webpage was https://data.gov.uk/datasets/gross-domestic-product. I’m looking at a nicely formatted webpage in my browser which describes the dataset with metadata about when it was released and who published it etc.\nTo load it into R, I want to be able to do:\nlibrary(readr)\n\ndf &lt;- read_csv(\"https://data.gov.uk/datasets/gross-domestic-product\")\nWith one line, the latest data is available for me to explore, and the URL is exactly the same as the URL used in my browser. I’m really inspired by this blog post by Ruben Verborgh where the case is made for using content negotiation, which enables this magic.\n\nIn open data circles, the FAIR principles are frequently discussed. FAIR stands for Findable, Accessible, Interoperable, and Reusable.\nThe Data on the Web Best Practices offer recommendations similar to the FAIR principles, and I’m a big fan of this document. These recommendations were developed based on use cases and requirements collected from open data users.\nTo summarise broadly, the advice boils down to:\n\nChoose a globally unique and persistent identifier for a dataset.\nProvide metadata, including information about structure, provenance, licences, etc.\nDo dataset versioning well.\nBe a good web citizen by leveraging HTTP and offering multiple formats.\n\nAdditionally, I recommend ensuring that if you provide data in a tabular format (like CSV), it should be formatted as tidy data.\nI’ll explain each of these points in more detail."
  },
  {
    "objectID": "blog/my-ideal-data-api/index.html#choose-a-globally-unique-and-persistent-identifier",
    "href": "blog/my-ideal-data-api/index.html#choose-a-globally-unique-and-persistent-identifier",
    "title": "My ideal open data API",
    "section": "Choose a globally unique and persistent identifier",
    "text": "Choose a globally unique and persistent identifier\nIt’s recommended to use a globally unique and persistent identifier for a dataset to ensure that it can always be reliably found and referenced over time. By giving a unique ID to each dataset, we avoid confusion and duplication, making it easier for researchers and systems to locate and access the exact dataset we’re referring to.\nA persistent identifier means the link to the dataset will not break or change, even if the dataset moves to a different location or is updated. Cool URIs don’t change.\nWe carefully considered what the URL scheme should look like and ended up with something roughly like this:\n\nhttps://data.gov.uk\nhttps://data.gov.uk/datasets\nhttps://data.gov.uk/themes/economy\nhttps://data.gov.uk/datasets/gross-domestic-product\nhttps://data.gov.uk/datasets/gross-domestic-product/editions/2024-05\nhttps://data.gov.uk/datasets/gross-domestic-product/editions/2024-05.csv\nhttps://data.gov.uk/datasets/gross-domestic-product/editions/2024-05.json\nhttps://data.gov.uk/datasets/gross-domestic-product/editions/2024-05/versions/1\n\nThe main features of these URLs are:\n\nWe pay attention to the URL structure and naming conventions, as they are permanent.\nThe URLs use kebab-case, which is beneficial for search engine optimisation.\nThe URLs are designed to be fairly human-readable.\nURLs for related resources build upon one another in a hierarchical structure.\nWe use ISO standards for years, quarters, months, etc.\nThe URLs for datasets don’t contain the theme or other descriptive metadata, as this may change or a dataset may have multiple themes.\nWe follow the principle that Cool URIs don’t change.\n\n\nMany data APIs require us to use a different URL to access a dataset from the URL of the webpage that describes the dataset. So which one of these URLs truly uniquely represents the dataset? Which one should show up in Google searches, or be used in academic papers?\nOne of the big debating points was whether it’s significantly more effort for users to request data from:\n\nhttps://api.data.gov.uk/v2/datasets/gross-domestic-product/edition/2024-05/csv\n\ninstead of:\n\nhttps://data.gov.uk/datasets/gross-domestic-product.csv\nhttps://data.gov.uk/datasets/gross-domestic-product/edition/2024-05.csv\n\nand I’d argue that yes, it is:\n\nSharing the first URL with a colleague loses the direct connection to the user-friendly webpage that describes the dataset. There is no obvious relationship between this API URL and the URL that users would see on a search engine, which makes it harder to find and understand the dataset.\nUsing an api subdomain, including an API version like /v2/, and requiring users to specify a dataset edition complicates the process of quickly assessing if the data is suitable – users end up mutating URLs by hand.\nAdditionally, including an API version such as /v2/ implies that the dataset’s identifier might change with future API versions, which means the URL might not remain persistent.\n\nMy aim was to try and work a bit harder behind the scenes, so we didn’t have to pass the complexity of manipulating URLs to the user."
  },
  {
    "objectID": "blog/my-ideal-data-api/index.html#use-tidy-data",
    "href": "blog/my-ideal-data-api/index.html#use-tidy-data",
    "title": "My ideal open data API",
    "section": "Use tidy data",
    "text": "Use tidy data\nProviding data in a presentational format, such as a spreadsheet, can be helpful to read the data but difficult to reuse the data. Data needs to be available as tidy data in another format such as CSV in order to be reusable. Robin Linacre argues for parquet (and makes a lot of other great points I agree with!)\nConsider this example taken from the RDF data cube vocabulary, which describes life expectancy broken down by region, sex and time:\n# An example of how the table looks once imported into R:\n\n# A tibble: 6 x 7\n  V1                V2        V3        V4        V5     V6    V7  \n  &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; \n1 \"\"                2004-2006 2005-2007 2006-2008 NA     NA    NA  \n2 \"\"                Male      Female    Male      Female Male  Female\n3 \"Newport\"         76.7      80.7      77.1      80.9   77.0  81.5  \n4 \"Cardiff\"         78.7      83.3      78.6      83.7   78.7  83.4  \n5 \"Monmouthshire\"   76.6      81.3      76.5      81.5   76.6  81.7  \n6 \"Merthyr Tydfil\"  75.5      79.1      75.5      79.4   74.9  79.6  \nThe table is a cross tabulation of the data, with the columns representing the time period of the observation and the sex of the observed population and the rows representing different locations. Having multiple header rows which span multiple columns makes the data difficult to read with software. Downstream users of the data will have to wrangle the data into a usable format.\nImporting the above table into a statistical software such as R produces a result with some problems:\n\nThe header rows are not treated as headers\nThe header row representing time period is not fully populated\nThe first column contains empty strings\nNumbers in the data are treated as strings, due to columns having mixed data types\n\nOrganising the table as tidy data, with each variable having its own column gives an output which can be instantly read into R, without need for further cleaning.\nFor data to be classified as tidy data:\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\n# A tibble: 24 × 4\n   area    sex    period    life_expectancy\n   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;               &lt;dbl&gt;\n 1 Newport Male   2004-2006            76.7\n 2 Newport Female 2004-2006            80.7\n 3 Newport Male   2005-2007            77.1\n 4 Newport Female 2005-2007            80.9\n 5 Newport Male   2006-2008            77.0\n 6 Newport Female 2006-2008            81.5\n 7 Cardiff Male   2004-2006            78.7\n 8 Cardiff Female 2004-2006            83.3\n 9 Cardiff Male   2005-2007            78.6\n10 Cardiff Female 2005-2007            83.7\n# ℹ 14 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "blog/my-ideal-data-api/index.html#do-dataset-versioning-well",
    "href": "blog/my-ideal-data-api/index.html#do-dataset-versioning-well",
    "title": "My ideal open data API",
    "section": "Do dataset versioning well",
    "text": "Do dataset versioning well\nThe Data Catalog Vocabulary (DCAT) - Version 3 is currently going through its finalisation process, and one of the updates in the third version of the standard was the introduction of a DatasetSeries and some guidance on how to handle versioning of datasets.\nThe way DCAT describes things in its example, it organises datasets into three layers.\n\nA DatasetSeries at the top, which groups together related datasets which are released at a regular cadence (such as the Gross Domestic Product).\nA set of Datasets which form part of the series. We referred to these as editions of a dataset series.\nA set of Datasets which are the various versions of a particular edition.\n\nAs a diagram I imagine it like this:\n\n\n\n\n\ngraph TD\n    A[DatasetSeries: GDP] --&gt; B[Edition: GDP July]\n    B --&gt; G[Version: GDP July v1]\n    A --&gt; C[Edition: GDP August]\n    C --&gt; H[Version: GDP August v1]\n    A --&gt; D[Edition: GDP September]\n    D --&gt; E[Version: GDP September v1]\n    D --&gt; F[Version: GDP September v2]\n\n\n\n\n\n\nThis allows us the ability to differentiate between releases of new data which are on a scheduled and expected basis (e.g. when we update each month with new data), vs. when we need to make a correction to a dataset due to some unexpected error or revision.\nIn terms of the identifiers for all of these resources, I imagined them looking like:\n\nhttps://data.gov.uk/datasets/gross-domestic-product\n\nhttps://data.gov.uk/datasets/gross-domestic-product/editions/2023-09\n\nhttps://data.gov.uk/datasets/gross-domestic-product/editions/2023-09/versions/1\nhttps://data.gov.uk/datasets/gross-domestic-product/editions/2023-09/versions/2\n\n\n\nAdditionally, I felt like an API could make a helpful assumption for users who didn’t specify a request for a specific edition or version. Should a user request https://data.gov.uk/datasets/gross-domestic-product.csv, it seemed like a sensible default would be to route the user through to the latest version of the latest edition – in this instance https://data.gov.uk/datasets/gross-domestic-product/editions/2023-09/versions/2.csv.\n\nAPI versioning\nAnother big debating point related to versioning was whether an API version identifier should feature in the identifier given to a dataset. My feeling is that the canonical identifier for a dataset should be a URL which does not have an API version identifier in it.\nSo ultimately, it can be fine to offer https://data.gov.uk/v2/datasets/gross-domestic-product, but only if the canonical URL, https://data.gov.uk/datasets/gross-domestic-product, works too."
  },
  {
    "objectID": "blog/my-ideal-data-api/index.html#provide-metadata",
    "href": "blog/my-ideal-data-api/index.html#provide-metadata",
    "title": "My ideal open data API",
    "section": "Provide metadata",
    "text": "Provide metadata\nMany analysts use Google and other search engines to find information. By using good metadata and following standards, we make it easier for them to find and understand the data. The Data on the Web Best Practices gives a few different types of metadata which we should provide: descriptive metadata, structural metadata, licensing, provenance and data quality information being key examples.\nThere’s a few standards in this space which are of particular interest:\n\nJSON-LD\nData Catalog Vocabulary (DCAT) - Version 3\nSchema.org\nCSV on the Web (CSVW)\n\nGoogle’s guidance shows how to embed JSON-LD in a webpage’s HTML to add structured data about a dataset. This structured data can be used for dataset catalogs or other widgets. For example, searching Google for the UK’s GDP brings up an interactive chart at the top of the results, providing a quick and intuitive way to understand the data and get an answer.\nSearch engines typically recommend using Schema.org, but Google’s guidance suggests they also supports DCAT. We believed DCAT has a more developed vocabulary for describing dataset metadata, so I prefer using it over Schema.org in this instance. The DCAT standard gives a mapping between DCAT and Schema.org.\n\nWe can understand structured data in web pages about datasets, using either schema.org Dataset markup, or equivalent structures represented in W3C’s Data Catalog Vocabulary (DCAT) format. We also are exploring experimental support for structured data based on W3C CSVW, and expect to evolve and adapt our approach as best practices for dataset description emerge. For more information about our approach to dataset discovery, see Making it easier to discover datasets.\n\nGoogle and the Data on the Web Best Practices also mention the CSV on the Web (CSVW) standard, which allows us to provide structural metadata about tabular datasets such as those found in CSV files. We were also interested in CSVW because it allows us to map tabular data into RDF. We had some success creating RDF data cubes using the RDF data cube vocabulary, and this approach has a lot of potential, but it requires significant effort and technical knowledge.\nJSON-LD is a way to organise data that makes it easier for different systems to understand and use it together. We can create a JSON-LD @context to name the keys in our JSON however we like, for example, using “summary” instead of “abstract”, while remaining interoperable with other metadata sources. The @context will map these keys back to the identifiers which are part of the metadata standards we’re using. This also allows us to provide interoperable responses in different languages, such as Welsh.\n\nBringing these standards together, I imagined a metadata response for a dataset series looking something like this:\n{\n    \"@context\": \"https://data.gov.uk/ns#\",\n    \"@id\": \"https://data.gov.uk/datasets/gross-domestic-product\",\n    \"@type\": \"dcat:DatasetSeries\",\n    \"identifier\": \"gdp\",\n    \"title\": \"Gross Domestic Product (GDP)\",\n    \"summary\": \"Gross Domestic Product (GDP) is the total monetary value of all goods and services produced within a country's borders in a specific time period.\",\n    \"description\": \"Gross Domestic Product (GDP) is a comprehensive measure of a nation's overall economic activity. It represents the total monetary value of all goods and services produced within a country's borders in a specific time period, typically annually or quarterly.\",\n    \"issued\": \"2023-07-21T00:07:00+01:00\",\n    \"next_release\": \"2023-10-20T00:07:00+01:00\",\n    \"publisher\": \"office-for-national-statistics\",\n    \"creator\": \"office-for-national-statistics\",\n    \"contact_point\": {\n        \"name\": \"Gross Domestic Product Enquiries\",\n        \"email\": \"gdp@data.gov.uk\"\n    },\n    \"themes\": [\n        \"economy\"\n    ],\n    \"frequency\": \"monthly\",\n    \"keywords\": [\n        \"gdp\",\n        \"inflation\",\n        \"gross domestic product\"\n    ],\n    \"licence\": \"http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/\",\n    \"spatial_coverage\": \"K02000001\",\n    \"temporal_coverage\": {\n        \"start\": \"1989-01-01T00:00:00+00:00\",\n        \"end\": \"2023-09-01T00:00:00+01:00\"\n    },\n    \"temporal_resolution\": \"P1M\",\n    \"editions\": [\n        {\n            \"@id\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09\",\n            \"issued\": \"2023-09-21T00:07:00+01:00\",\n            \"modified\": \"2023-09-22T00:07:00+01:00\"\n        },\n        {\n            \"@id\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-08\",\n            \"issued\": \"2023-08-21T00:07:00+01:00\",\n            \"modified\": \"2023-08-21T00:07:00+01:00\"\n        },\n        {\n            \"@id\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-07\",\n            \"issued\": \"2023-07-21T00:07:00+01:00\",\n            \"modified\": \"2023-07-21T00:07:00+01:00\"\n        }\n    ]\n}\nUnder editions we’ve avoided a load of nested JSON and instead chosen to deliver a subset of important metadata items to the user. A user has enough information to make a decision about whether they should enquire further and make another request.\nThis was also a debating point, as I think in other API designs this sort of nesting seems to be frowned upon. In json:api or HAL, additional resources are listed under a _links keyword with the idea that the user would make additional calls to the API to get further information. This approach makes sense for software engineers writing integrations, as it keeps responses small and standardised. However, I personally appreciate having a bit of additional nested metadata to help me decide whether to make an additional request.\n\nA request for metadata about a particular edition of a dataset would look very similar and likely reuse much of the metadata from the data series. Some core differences would include:\n\nWe’d include some of the structural metadata.\nWe’d include information about versions and distributions, rather than editions.\n\nI imagined a response looking like this:\n{\n    \"@context\": \"https://data.gov.uk/ns#\",\n    \"@id\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09\",\n    \"@type\": \"dcat:Dataset\",\n    \"identifier\": \"gdp-2023-09\",\n    \"title\": \"Gross Domestic Product (GDP): September 2023\",\n    \"summary\": \"Gross Domestic Product (GDP) is the total monetary value of all goods and services produced within a country's borders in a specific time period.\",\n    \"description\": \"Gross Domestic Product (GDP) is a comprehensive measure of a nation's overall economic activity. It represents the total monetary value of all goods and services produced within a country's borders in a specific time period, typically annually or quarterly.\",\n    \"issued\": \"2023-09-21T00:07:00+01:00\",\n    \"modified\": \"2023-09-22T00:07:00+01:00\",\n    \"next_release\": \"2023-10-20T00:07:00+01:00\",\n    \"publisher\": \"office-for-national-statistics\",\n    \"creator\": \"office-for-national-statistics\",\n    \"contact_point\": {\n        \"name\": \"Gross Domestic Product Enquiries\",\n        \"email\": \"gdp@data.gov.uk\"\n    },\n    \"themes\": [\n        \"economy\"\n    ],\n    \"frequency\": \"monthly\",\n    \"keywords\": [\n        \"gdp\",\n        \"inflation\",\n        \"gross domestic product\"\n    ],\n    \"licence\": \"http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/\",\n    \"spatial_coverage\": \"K02000001\",\n    \"temporal_coverage\": {\n        \"start\": \"1989-01-01T00:00:00+00:00\",\n        \"end\": \"2023-09-01T00:00:00+01:00\"\n    },\n    \"temporal_resolution\": \"P1M\",\n    \"version\": 2,\n    \"current_version\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09/version/2\",\n    \"versions\": [\n        {\n            \"@id\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09/version/1\",\n            \"issued\": \"2023-09-21T00:07:00+01:00\",\n            \"modified\": \"2023-09-21T00:07:00+01:00\",\n            \"version_notes\": \"This version was replaced following the correction of an error in the September 2023 data.\"\n        },\n        {\n            \"@id\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09/version/2\",\n            \"issued\": \"2023-09-22T00:07:00+01:00\",\n            \"modified\": \"2023-09-22T00:07:00+01:00\",\n        },\n    ],\n    \"distributions\": [\n        {\n            \"@id\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09.csv\",\n            \"@type\": [\"dcat:Distribution\", \"csvw:Table\"],\n            \"url\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09.csv\",\n            \"download_url\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09.csv\",\n            \"csvw_metadata\": \"https://data.gov.uk/datasets/gross-domestic-product/2023-09.csv-metadata.json\",\n            \"media_type\": \"text/csv\",\n            \"table_schema\": {\n                \"columns\": [\n                    {\n                        \"title\": \"geography\",\n                        \"datatype\": \"string\",\n                        \"description\": \"The geographic area covered by the index.\"\n                    },\n                    {\n                        \"title\": \"time_period\",\n                        \"datatype\": \"string\",\n                        \"description\": \"The time period covered by the index.\"\n                    },\n                    {\n                        \"title\": \"gross_domestic_product\",\n                        \"datatype\": \"decimal\",\n                        \"description\": \"The value of the Gross Domestic Product.\"\n                    }\n                ]\n            }\n        }\n    ]\n}\nHere we’re embedding CSVW structural metadata within a wider JSON-LD document. The CSVW standard gives quite a specific specification of how a CSVW metadata file needs to look and be structured, and our experience was that it was a bit constraining. But if we store our metadata in a triple store, we should be able to construct a file which matches that described by the spec, or to do things like dynamically translate between using DCAT and Schema.org.\nWhen we were creating RDF data cubes, I felt it was most natural metadata about these to add these as an additional distribution, so a dataset edition could have both a CSV representation and an RDF data cube representation."
  },
  {
    "objectID": "blog/my-ideal-data-api/index.html#be-a-good-web-citizen",
    "href": "blog/my-ideal-data-api/index.html#be-a-good-web-citizen",
    "title": "My ideal open data API",
    "section": "Be a good web citizen",
    "text": "Be a good web citizen\nI briefly mentioned content negotiation and this blog post by Ruben Verborgh. Given that we have assigned a unique identifier to a dataset, such as https://data.gov.uk/datasets/gross-domestic-product, it would be beneficial to use web standards to allow users to request different representations of that dataset – whether they want an HTML webpage, a CSV, or a JSON format.\nThe idea is that when you request https://data.gov.uk/datasets/gross-domestic-product from a web browser, the browser asks for an HTML page that is suitable for human consumption. But from our statistical tools, users could request a representation like CSV. Both tools use the same unique identifier, but ask for different representations.\nBy doing this, we ensure the dataset has a single identifier; a single URL that is indexed by Google, can be cited in academic papers and used in code without awkward URL manipulations.\nImplementing content negotiation does require extra effort. I noticed a recent weeknote from parliament.gov.uk where they faced issues with incorrect resource caching, but it’s great to see their attempts to provide this functionality and weighing up their options.\nBut for what it’s worth, I also think appending the filetype to the URL is reasonable. Requesting https://data.gov.uk/datasets/gross-domestic-product.csv and receiving a CSV would likely be seen as helpful. If I wanted the dataset’s data and metadata as a JSON, I could ask for https://data.gov.uk/datasets/gross-domestic-product.json instead.\nThis approach introduces different identifiers with distinct meanings:\n\nhttps://data.gov.uk/datasets/gross-domestic-product represents a dataset, which is an abstract resource with no specific serialisation or representation.\nhttps://data.gov.uk/datasets/gross-domestic-product.csv represents the CSV distribution of that dataset.\nhttps://data.gov.uk/datasets/gross-domestic-product.json represents the JSON distribution of that dataset."
  },
  {
    "objectID": "blog/my-ideal-data-api/index.html#final-words",
    "href": "blog/my-ideal-data-api/index.html#final-words",
    "title": "My ideal open data API",
    "section": "Final words",
    "text": "Final words\nWith so many new services looking to solve similar problems, I hope these thoughts are helpful to others and show what inspired me as I was thinking how to provide a good dataset API to users.\nIf nothing else, I’d highlight that the Data on the Web Best Practices is an incredible resource for those of us trying to make data more easily available to consumers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello!",
    "section": "",
    "text": "{\n  function binomialRandom(n, p) {\n    let x = 0;\n    for (let i = 0; i &lt; n; i++) {\n      if (Math.random() &lt; p) {\n        x++;\n      }\n    }\n    return x;\n  }\n\n  function generateData(particles, places) {\n    const data = [];\n\n    particles.forEach((value, index) =&gt; {\n      for (let y = 1; y &lt;= value; y++) {\n        const colour = `hsl(${Math.floor(Math.random() * 360)}, 70%, 70%)`;\n        data.push({ x: index - Math.floor(places / 2), y: y, colour: colour });\n      }\n    });\n\n    return data;\n  }\n\n  function leap(data, places, maxParticles) {\n    const maxYs = data.reduce((acc, p) =&gt; {\n      if (!acc[p.x] || p.y &gt; acc[p.x].y) {\n        acc[p.x] = p;\n      }\n      return acc;\n    }, {});\n\n    const topmostParticles = Object.values(maxYs);\n    const particleToModifyIndex = Math.floor(\n      Math.random() * topmostParticles.length\n    );\n    const particleToModify = topmostParticles[particleToModifyIndex];\n\n    const minX = Math.ceil(-(places + 1) / 2);\n    const maxX = Math.floor((places + 1) / 2);\n\n    let newX;\n    if (particleToModify.x === minX) {\n      newX = particleToModify.x + 1;\n    } else if (particleToModify.x === maxX) {\n      newX = particleToModify.x - 1;\n    } else {\n      newX = particleToModify.x + (Math.random() &lt; 0.5 ? -1 : 1);\n      if (newX &lt; minX) newX = minX;\n      if (newX &gt; maxX) newX = maxX;\n    }\n\n    const maxYAtNewX =\n      Math.max(0, ...data.filter((p) =&gt; p.x === newX).map((p) =&gt; p.y)) + 1;\n    if (maxYAtNewX &gt; maxParticles) {\n      return data;\n    }\n\n    data.forEach((p) =&gt; {\n      if (p === particleToModify) {\n        p.x = newX;\n        p.y = maxYAtNewX;\n      }\n    });\n\n    return data;\n  }\n\n  function createSvg(width, height, margin) {\n    return d3.create(\"svg\").attr(\"viewBox\", [0, 0, width, height]);\n  }\n\n  function createScales(places, maxParticles, width, height, margin) {\n    const x = d3\n      .scaleLinear()\n      .domain([-(places + 1) / 2, (places + 1) / 2])\n      .range([margin.left, width - margin.right]);\n\n    const y = d3\n      .scaleLinear()\n      .domain([0, maxParticles + 1])\n      .range([height - margin.bottom, margin.top]);\n\n    return { x, y };\n  }\n\n  function createXAxis(g, x, height, margin, places) {\n    g.attr(\"transform\", `translate(0,${height - margin.bottom})`).call(\n      d3.axisBottom(x).ticks(places).tickSizeOuter(0)\n    );\n  }\n\n  function drawParticles(svg, data, x, y, radius) {\n    return svg\n      .append(\"g\")\n      .selectAll(\"circle\")\n      .data(data)\n      .join(\"circle\")\n      .attr(\"cx\", (d) =&gt; x(d.x))\n      .attr(\"cy\", (d) =&gt; y(d.y))\n      .attr(\"r\", radius)\n      .attr(\"fill\", (d) =&gt; d.colour);\n  }\n\n  function animate(data, places, maxParticles, circles, x, y, duration) {\n    setInterval(() =&gt; {\n      const newData = leap(data, places, maxParticles);\n\n      circles = circles.data(newData).join(\n        (enter) =&gt;\n          enter\n            .append(\"circle\")\n            .attr(\"cx\", (d) =&gt; x(d.x))\n            .attr(\"cy\", (d) =&gt; y(d.y))\n            .attr(\"fill\", (d) =&gt; d.colour)\n            .call((enter) =&gt; enter.transition().duration(duration)),\n        (update) =&gt;\n          update.call((update) =&gt;\n            update\n              .transition()\n              .duration(duration)\n              .attr(\"cx\", (d) =&gt; x(d.x))\n              .attr(\"cy\", (d) =&gt; y(d.y))\n              .attr(\"fill\", (d) =&gt; d.colour)\n          ),\n        (exit) =&gt;\n          exit.call((exit) =&gt; exit.transition().duration(duration).remove())\n      );\n    }, duration);\n  }\n\n  function main() {\n    const width = window.innerWidth &lt; 768 ? 300 : 400;\n    const height = 100\n    const places = 10;\n    const maxParticles = 3;\n    const radius = height / 25;\n    const margin = { top: 20, right: 30, bottom: 40, left: 40 };\n    const p = 0.4;\n    const duration = 500;\n\n    const particles = Array.from({ length: places + 1 }, () =&gt;\n      binomialRandom(maxParticles, p)\n    );\n    const data = generateData(particles, places);\n\n    const svg = createSvg(width, height, margin);\n    const { x, y } = createScales(places, maxParticles, width, height, margin);\n    svg.append(\"g\").call((g) =&gt; createXAxis(g, x, height, margin, places));\n\n    let circles = drawParticles(svg, data, x, y, radius);\n    animate(data, places, maxParticles, circles, x, y, duration);\n\n    return svg.node();\n  }\n\n  return main();\n}\n\n\n\n\n\n\n\nHello!\nI’m Ross. I’m Head of Analytics at Companies House 📈🏡📊.\nI like Bayesian statistics, multilevel models and R. I think confidence intervals are overrated and misunderstood.\nHere’s some of my recent stuff:\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n2025-05-11\n\n\nShut the box!\n\n\n\n\n2025-04-22\n\n\nQUALIFY in SQL\n\n\n\n\n2024-06-30\n\n\nMy ideal open data API\n\n\n\n\n2024-05-28\n\n\nExploring expert systems\n\n\n\n\n2023-10-31\n\n\nHow long will my machine last?\n\n\n\n\n2023-09-06\n\n\nConfidence intervals in the wild\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/shut-the-box/index.html",
    "href": "blog/shut-the-box/index.html",
    "title": "Shut the box!",
    "section": "",
    "text": "A few years ago I went to Thailand with some friends. We were hanging out in a bar one night and ended up playing this game called Shut the Box.\nThe board has numbers 1 to 9. Each turn, you roll two dice. You can then flip or “shut” any combination of numbers which sum to the total of the two dice. The game ends when you can no longer make a valid move. You win the game if no numbers are left to shut, otherwise you score the sum of the remaining open numbers.\nWe all played and started to develop our own strategies. I knew that the sum of two dice is most likely to be 7 and convinced myself that as 7 was the most likely sum and 1, 2, 3 and 4 were the least likely, I should prioritise shutting 1, 2, 3 and 4.\nMy friend’s strategy was to prioritise 7, 8 and 9 if it was possible to shut them, and if not, to prioritise the smallest numbers. He did much better and my intuition was completely wrong! But I’ve always wondered if I could find the mathematically optimal strategy.\nThinking about this as a stochastic optimisation problem, we can model the game as a Markov decision process."
  },
  {
    "objectID": "blog/shut-the-box/index.html#markov-decision-processes",
    "href": "blog/shut-the-box/index.html#markov-decision-processes",
    "title": "Shut the box!",
    "section": "Markov decision processes",
    "text": "Markov decision processes\nA Markov decision process models the evolution of a system over time. At each time step, an agent makes a decision that affects the future state of the system.\nThe process has a state space \\(S\\) and evolves in discrete time steps \\(t = 1, 2 \\ldots\\)\nAt each time step, the agent chooses from a set of possible actions. Typically, the set of available actions depends on the current state \\(s \\in S\\), and is denoted \\(A(s)\\).\nGiven a state \\(s \\in S\\) and an action \\(a \\in A(s)\\), the system transitions to a new state \\(s'\\) according to a transition probability \\(P(s' \\mid s, a)\\). Each transition gives a reward \\(R(s, s', a)\\). These transition probabilities are Markovian – the next state depends only on the current state and action, not on the history of past states and actions.\nThe typical goal of the agent is to choose actions that maximise the expected cumulative reward over time. This means finding a policy \\(\\pi(s)\\) – a probabilistic mapping of states to actions – which maximises the expected total reward."
  },
  {
    "objectID": "blog/shut-the-box/index.html#our-model",
    "href": "blog/shut-the-box/index.html#our-model",
    "title": "Shut the box!",
    "section": "Our model",
    "text": "Our model\nFor a game of Shut the Box, the state space \\(S\\) consists of all possible subsets of the numbers 1 to 9. The state represents the set of numbers that are not shut. The game starts with all numbers open, \\(s = \\{1, 2, \\ldots, 9\\}\\).\nThe available actions at state \\(s\\) depend on the total \\(d\\) of two dice. \\(A_d(s)\\) is the set of all subsets of \\(s\\) whose elements sum to \\(d \\in \\{ 2, 3, \\ldots, 12\\}\\). The chosen action \\(a \\in A_d(s)\\) represents the numbers to shut on that turn.\nFor example, say the remaining numbers were \\(s = \\{1, 2, 3, 4, 5\\}\\), and we roll \\(d = 8\\), we could shut \\(A_8(s) = \\{\\{3, 5\\}\\), \\(\\{1, 2, 5\\}\\), or \\(\\{1, 3, 4\\}\\}\\).\nThe transitions between different states are deterministic based on this choice, so \\(s' = s \\setminus a\\).\nWe play over the discrete time steps \\(t = 1, 2, \\ldots\\) and the game ends when there are no valid moves left, \\(A_d(s) = \\emptyset\\). At that point a cost is incurred: \\[R(s) = \\sum_{i \\in s} i.\\]\nOur goal is to find a policy \\(\\pi(s, d)\\) that minimises the expected cost."
  },
  {
    "objectID": "blog/shut-the-box/index.html#solution",
    "href": "blog/shut-the-box/index.html#solution",
    "title": "Shut the box!",
    "section": "Solution",
    "text": "Solution\nLet \\(V(s)\\) be the expected cost at the end of the game when proceeding from state \\(s\\).\nFor a given dice roll \\(d\\), define: \\[\nV_d(s) =\n\\begin{cases}\n\\min\\limits_{a \\in A_d(s)} V(s \\setminus a) & \\text{if } A_d(s) \\ne \\emptyset, \\\\\n\\sum\\limits_{i \\in s} i & \\text{otherwise}.\n\\end{cases}\n\\]\nThen the overall expected cost is: \\[V(s) = \\sum_{d=2}^{12} P(d) \\cdot V_d(s)\\]\nwhere \\(P(d)\\) is the probability of rolling two dice which sum to \\(d\\).\nIf no valid actions remain, then \\(V(s)\\) is equivalent to the final reward: \\[V(s) = R(s) = \\sum_{i \\in s} i.\\]\nUsing dynamic programming, we can compute the expected cost for each state \\(s\\) by iterating over all possible states and actions.\nFirst we create a function to compute the probability of rolling two dice which sum to \\(d\\):\n\nddice &lt;- function(x) {\n  probs &lt;- c(1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1) / 36\n  out &lt;- numeric(length(x))\n  \n  # Return 0 if the element is not in 2 to 12\n  is_valid &lt;- x &gt;= 2 & x &lt;= 12\n  out[is_valid] &lt;- probs[x[is_valid] - 1]\n  \n  return(out)\n}\n\nddice(1:12)\n#&gt;  [1] 0.00000000 0.02777778 0.05555556 0.08333333 0.11111111 0.13888889\n#&gt;  [7] 0.16666667 0.13888889 0.11111111 0.08333333 0.05555556 0.02777778\n\nNext we create a function to compute the set of valid actions for a given state and dice total:\n\nlibrary(purrr)\n\nvalid_actions &lt;- function(s, d) {\n  if (d &lt; 2 || d &gt; 12) {\n    stop(\"Dice total must be between 2 and 12.\")\n  }\n  \n  if (is.null(s) || all(is.na(s))) {\n    return(NA)\n  }\n  \n  # The combn function doesn't handle vectors of length 1 as we intended, so\n  # we handle the special case here\n  if (length(s) == 1) {\n    return(if (s == d) list(s) else NA)\n  }\n  \n  subsets &lt;- map(1:length(s), function(x) combn(s, x, simplify = FALSE)) |&gt;\n    flatten()\n  \n  valid_subsets &lt;- keep(subsets, function(x) sum(x) == d)\n\n  if (length(valid_subsets) == 0) {\n    return(NA)\n  }\n  \n  return(valid_subsets)\n}\n\n\nvalid_actions(1:5, 8)\n#&gt; [[1]]\n#&gt; [1] 3 5\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 1 2 5\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 1 3 4\n\nWe can implement the value function to compute the expected cost of a given state. We use memoisation to speed up the computation:\n\nlibrary(memoise)\n\nvalue &lt;- function(s) {\n\n  # If the state is empty, no cost\n  if (is.null(s) || all(is.na(s))) {\n    return(0)\n  }\n\n  dice_values &lt;- 2:12\n  dice_probs &lt;- ddice(dice_values)\n\n  # For each dice roll, calculate the min value of next state if valid actions exist\n  expected_costs &lt;- map2_dbl(dice_values, dice_probs, function(d, p) {\n    actions &lt;- valid_actions(s, d)\n\n    if (is.null(actions) || all(is.na(actions))) {\n      # No valid action, the game ends and we incur cost\n      return(p * sum(s))\n    }\n\n    # Otherwise, take the best (minimal) value among next states\n    min_cost &lt;- map_dbl(actions, function(a) value(setdiff(s, a))) |&gt; min()\n    return(p * min_cost)\n  })\n\n  # Expected cost is weighted average over all dice outcomes\n  sum(expected_costs)\n}\n\nvalue &lt;- memoise(value)\n\nvalue(8)\n#&gt; [1] 6.888889\n\nvalue(c(2, 6))\n#&gt; [1] 5.969136\n\nAnd finally, for a given state and dice total, we can compute the optimal action to take:\n\noptimal_action &lt;- function(s, d) {\n  actions &lt;- valid_actions(s, d)\n\n  if (is.null(actions) || all(is.na(actions))) {\n      return(NA)\n  }\n\n  # Evaluate each possible next state and pick the one with the lowest value\n  values &lt;- map_dbl(actions, function(a) value(setdiff(s, a)))\n  best_index &lt;- which.min(values)\n\n  return(actions[[best_index]])\n}\n\noptimal_action(1:5, 8)\n#&gt; [1] 3 5"
  },
  {
    "objectID": "blog/shut-the-box/index.html#results",
    "href": "blog/shut-the-box/index.html#results",
    "title": "Shut the box!",
    "section": "Results",
    "text": "Results\nWith some help from Copilot we get a function to simulate a game of Shut the Box:\n\n\nCode\nsimulate_game &lt;- function(state = 1:9) {\n  cat(\"🎲 Starting a game of Shut the Box!\\n\")\n  cat(\"Tiles open:\", paste0(\"{\", paste(state, collapse = \", \"), \"}\"), \"\\n\\n\")\n  \n  turn &lt;- 1\n  \n  while (length(state) &gt; 0) {\n    cat(glue::glue(\"Turn {turn}\"))\n    turn &lt;- turn + 1\n\n    # Roll two dice\n    dice_values &lt;- 2:12\n    d &lt;- sample(dice_values, 1, prob = ddice(dice_values))\n    cat(\"\\nRolled:\", d, \"\\n\")\n    \n    acts &lt;- valid_actions(state, d)\n    \n    if (length(acts) == 0) {\n      cat(\"No valid actions! Game over.\\n\")\n      cat(\"Tiles left:\", paste0(\"{\", paste(state, collapse = \", \"), \"}\"), \"\\n\")\n      cat(glue::glue(\"Final score (sum of remaining tiles): {sum(state)}\"))\n      cat(\"\\n\")\n      return(invisible(NULL))\n    }\n    \n    a &lt;- optimal_action(state, d)\n    cat(\"Action taken:\", paste0(\"{\", paste(a, collapse = \", \"), \"}\"), \"\\n\")\n    \n    state &lt;- setdiff(state, a)\n    cat(\"Tiles now:\", ifelse(length(state) == 0, \"(none - you've shut the box!)\", paste0(\"{\", paste(state, collapse = \", \"), \"}\")), \"\\n\\n\")\n  }\n  \n  cat(\"🏆 Congratulations, you've shut the box!\\n\")\n  cat(\"Final score: 0\\n\")\n}\n\nset.seed(123)\n\nsimulate_game()\n\n\n#&gt; 🎲 Starting a game of Shut the Box!\n#&gt; Tiles open: {1, 2, 3, 4, 5, 6, 7, 8, 9} \n#&gt; \n#&gt; Turn 1\n#&gt; Rolled: 8 \n#&gt; Action taken: {8} \n#&gt; Tiles now: {1, 2, 3, 4, 5, 6, 7, 9} \n#&gt; \n#&gt; Turn 2\n#&gt; Rolled: 10 \n#&gt; Action taken: {1, 9} \n#&gt; Tiles now: {2, 3, 4, 5, 6, 7} \n#&gt; \n#&gt; Turn 3\n#&gt; Rolled: 6 \n#&gt; Action taken: {6} \n#&gt; Tiles now: {2, 3, 4, 5, 7} \n#&gt; \n#&gt; Turn 4\n#&gt; Rolled: 11 \n#&gt; Action taken: {4, 7} \n#&gt; Tiles now: {2, 3, 5} \n#&gt; \n#&gt; Turn 5\n#&gt; Rolled: 3 \n#&gt; Action taken: {3} \n#&gt; Tiles now: {2, 5} \n#&gt; \n#&gt; Turn 6\n#&gt; Rolled: 7 \n#&gt; Action taken: {2, 5} \n#&gt; Tiles now: (none - you've shut the box!) \n#&gt; \n#&gt; 🏆 Congratulations, you've shut the box!\n#&gt; Final score: 0\n\n\nGreat news - we won a game using our optimal policy!\nAlthough we’ve got a way to programatically play the game optimally from each state, it’s still hard to know what the “rule” is without making use of some complicated look-up tables. I wanted to plot something, but the the board can have up to nine numbers open which makes it difficult to picture what’s going on in two or three dimensions.\nTo make things easier to understand, we group states by how many tiles remain open. We can then plot the expected cost for each state against the average open tile number:\n\n\nCode\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\n\n# Create a data frame with all possible states and their expected values\nall_states &lt;- map(1:9, function(x) combn(1:9, x, simplify = FALSE)) |&gt;\n  flatten() |&gt;\n  append(list(NA), after = 0)\n\npolicy_table &lt;- tibble(\n  state = all_states,\n  label = map_chr(all_states, function(x) paste0(\"{\", paste(x, collapse = \", \"), \"}\")),\n  value = map_dbl(all_states, value),\n)\n\n# Let's look at a sample of the data\npolicy_table |&gt; sample_n(10)\n#&gt; # A tibble: 10 × 3\n#&gt;    state     label           value\n#&gt;    &lt;list&gt;    &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 &lt;int [5]&gt; {1, 3, 5, 7, 9} 12.3 \n#&gt;  2 &lt;int [3]&gt; {4, 7, 8}       14.5 \n#&gt;  3 &lt;int [5]&gt; {1, 3, 4, 7, 8} 12.0 \n#&gt;  4 &lt;int [4]&gt; {3, 4, 7, 8}    15.2 \n#&gt;  5 &lt;int [4]&gt; {4, 5, 6, 9}    18.0 \n#&gt;  6 &lt;int [2]&gt; {1, 5}           4.61\n#&gt;  7 &lt;int [5]&gt; {3, 5, 6, 8, 9} 22.7 \n#&gt;  8 &lt;int [4]&gt; {1, 3, 4, 6}     5.43\n#&gt;  9 &lt;int [3]&gt; {2, 6, 7}        9.86\n#&gt; 10 &lt;int [3]&gt; {2, 6, 8}       12.5\n\npolicy_table |&gt; \n  filter(!is.na(state)) |&gt;\n  mutate(\n    state_average = map_dbl(state, mean),\n    tiles_open = map_dbl(state, length)\n  ) |&gt; \n  select(state_average, value, tiles_open) |&gt;\n  ggplot(aes(state_average, value)) +\n  geom_point() +\n  facet_wrap(vars(tiles_open), nrow = 3, labeller = \"label_both\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nThis definitely shows a pattern! States with higher average open tiles have higher expected costs. The costs are high, indicating some difficulty in shutting any tiles when the board is mostly made up of higher numbers. This supports the idea that we want to target higher numbers first.\nI was curious whether this holds across all states, for all dice rolls. Do we always prioritise the action which shuts the largest number?\nHere’s the instances where we don’t shut the largest number possible:\n\n\nCode\nlibrary(tidyr)\n\noptimal_actions &lt;- policy_table |&gt;\n  expand_grid(dice_total = 2:12) |&gt;\n  mutate(\n    actions = map2(state, dice_total, function(s, d) valid_actions(s, d)),\n    optimal_action = map2(state, dice_total, function(s, d) optimal_action(s, d))\n  )\n\noptimal_actions |&gt;\n  mutate(\n    valid_actions_max_element = map_dbl(\n      actions,\n      function(x) if (is.null(x) || all(is.na(x))) NA else max(unlist(x))\n    ),\n    valid_actions = map_chr(\n        actions,\n        function(x) if (is.null(x) || all(is.na(x))) NA else reduce(\n            x[-1], \n            function(y, z) paste0(y, \"; {\", paste(z, collapse = \", \"), \"}\"), \n            .init = paste0(\"{\", paste(x[[1]], collapse = \", \"), \"}\")\n        )\n    ),\n    optimal_action_max_element = map_dbl(\n      optimal_action,\n      function(x) if (is.null(x) || all(is.na(x))) NA else max(unlist(x))\n    ),\n    optimal_action = map_chr(\n      optimal_action,\n      function(x) if (is.null(x) || all(is.na(x))) NA else paste0(\n        \"{\", paste(unlist(x), collapse = \", \"), \"}\"\n      )\n    ),\n  ) |&gt;\n  filter(valid_actions_max_element != optimal_action_max_element) |&gt;\n  select(state = label, dice_total, valid_actions, optimal_action) |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\n\nI found myself staring at this for a while, particularly at the results for state \\(\\{1, 4, 5, 8\\}\\) given a dice roll of 9. In this scenario we can choose to close \\(\\{1, 8\\}\\) or \\(\\{4, 5\\}\\). Our computations suggest closing \\(\\{4, 5\\}\\).\n\nvalue(c(1, 8))\n#&gt; [1] 6.888889\n\nvalue(c(4, 5))\n#&gt; [1] 7.027778\n\nAt first, this result felt counterintuitive — I had expected that winning (shutting all tiles) from the state \\(\\{4, 5\\}\\) would be more likely than from \\(\\{1, 8\\}\\), and so this would give \\(\\{4, 5\\}\\) a lower expected cost.\nFrom the state \\(\\{4, 5\\}\\), we can only proceed if we roll a 4, 5, or 9. Any other dice roll results in an immediate loss:\n\\[\n\\begin{align}\nP(\\text{Win from } \\{4, 5\\}) &= P(\\text{Roll 4 and win from } \\{5\\}) \\\\\n&+ P(\\text{Roll 5 and win from } \\{4\\}) \\\\\n&+ P(\\text{Roll 9})\n\\end{align}\n\\]\nWe observe that:\n\\[\nP(\\text{Win from } \\{4\\}) = P(\\text{Roll } 4) = \\frac{3}{36},\n\\]\nand\n\\[\nP(\\text{Win from } \\{5\\}) = P(\\text{Roll } 5) = \\frac{4}{36}.\n\\]\nCombining these, we compute:\n\\[\nP(\\text{Win from } \\{4, 5\\}) = \\frac{3}{36} \\cdot \\frac{3}{36} + \\frac{4}{36} \\cdot \\frac{4}{36} + \\frac{4}{36} \\approx 0.13.\n\\]\nThe probability of winning from \\(\\{1, 8\\}\\) is the same as rolling a 9 (as that’s the only winning action), which is \\(\\frac{4}{36} \\approx 0.11\\). So it’s true – we’re slightly more likely to win the game from the state \\(\\{4, 5\\}\\). But our value function isn’t set up to maximise our probability of winning, it’s set up to minimise our expected cost.\nThe possibility of scoring 1 from state \\(\\{1, 8\\}\\) by rolling an 8 pulls the expected cost down, making it the better state to land on. The objectives of maximising winning chances and minimising expected costs diverge slightly!\nSo we’ve not managed to come up with a “rule” for people to follow, but it seems like we can do pretty well by closing fewer tiles, and prioritising closing higher numbers.\nFinally, to wrap up, what is the state with the highest expected cost?\n\npolicy_table |&gt;\n  arrange(desc(value)) |&gt;\n  select(label, value) |&gt;\n  head()\n#&gt; # A tibble: 6 × 2\n#&gt;   label              value\n#&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 {5, 6, 7, 8, 9}     24.7\n#&gt; 2 {4, 5, 7, 8, 9}     23.8\n#&gt; 3 {6, 7, 8, 9}        23.6\n#&gt; 4 {4, 5, 6, 7, 8, 9}  23.2\n#&gt; 5 {5, 7, 8, 9}        22.7\n#&gt; 6 {4, 6, 7, 8, 9}     22.7\n\nTurns out my original plan wasn’t just inoptimal, but the game state where 1, 2, 3 and 4 have all been shut has the highest expected cost. It’s the worst possible strategy!"
  },
  {
    "objectID": "blog/exploring-expert-systems/index.html",
    "href": "blog/exploring-expert-systems/index.html",
    "title": "Exploring expert systems",
    "section": "",
    "text": "I’ve been reading Data Patterns recently and I caught a line about trying to build a causal model of a business in order to understand it better.\nThis idea got me thinking about causal inference and I recalled a toy expert system used to determine if a patient has dyspnoea (shortness of breath). I found the original paper and decided to replicate the results using Stan.\nThe idea behind the system is:\nThis description leads to a causal Directed Acyclic Graph (DAG) that shows the dependencies among different traits.\nflowchart\n    A[visit to Asia?] --&gt; T[tuberculosis?]\n    S[smoking?] --&gt; L[lung cancer?]\n    S --&gt; B[bronchitis?]\n    T --&gt; E[either tuberculosis or lung cancer?]\n    L --&gt; E\n    E --&gt; X[positive X-ray?]\n    E --&gt; D[dyspnoea?]\n    B --&gt; D\nThese DAGs help us understand the dependencies between different traits and ask interesting questions about the system. The paper mentions several scenarios:\nWe aim to simulate data using the model and relationships described in the paper. From these samples, we can estimate the probabilities for each of the questions posed."
  },
  {
    "objectID": "blog/exploring-expert-systems/index.html#model-specification",
    "href": "blog/exploring-expert-systems/index.html#model-specification",
    "title": "Exploring expert systems",
    "section": "Model specification",
    "text": "Model specification\nThe paper provides several conditional probabilities:\n\nThe probability of someone visiting Asia is 0.01, and the probability of someone smoking is 0.5.\n\n\\[ P(\\text{asia} = 1) = 0.01 \\]\n\\[ P(\\text{smoking} = 1) = 0.5 \\]\n\nIf someone has visited Asia, the probability of tuberculosis is 0.05. If they have not visited Asia, the probability is 0.01.\n\n\\[ P(\\text{tuberculosis} = 1 \\mid \\text{asia} = 1) = 0.05 \\] \\[ P(\\text{tuberculosis} = 1 \\mid \\text{asia} = 0) = 0.01 \\]\n\nIf someone smokes, the probability of lung cancer is 0.1. If they do not smoke, the probability is 0.01.\n\n\\[ P(\\text{lung cancer} = 1 \\mid \\text{smoking} = 1) = 0.1 \\] \\[ P(\\text{lung cancer} = 1 \\mid \\text{smoking} = 0) = 0.01 \\]\n\nIf someone smokes, the probability of bronchitis is 0.6. If they do not smoke, the probability is 0.3.\n\n\\[ P(\\text{bronchitis} = 1 \\mid \\text{smoking} = 1) = 0.6 \\] \\[ P(\\text{bronchitis} = 1\\mid \\text{smoking} = 0) = 0.3 \\]\n\nHaving either tuberculosis or lung cancer is represented by the event “either”.\n\n\\[ \\{ \\text{either} = 1 \\} = \\{ \\text{tuberculosis} = 1 \\} \\vee \\{ \\text{lung cancer} = 1 \\} \\]\n\nIf someone has either tuberculosis or lung cancer, the probability of a positive x-ray is 0.98. If they do not have either, the probability is 0.05. This can be interpreted as the x-ray having 98% sensitivity and 95% specificity.\n\n\\[ P(\\text{x-ray} = 1 \\mid \\text{either} = 1) = 0.98 \\] \\[ P(\\text{x-ray} = 1 \\mid \\text{either} = 0) = 0.05 \\]\n\nFinally, if someone has either tuberculosis or lung cancer and also has bronchitis, the probability of dyspnoea is 0.9. If they have either but not bronchitis, the probability is 0.7. If they do not have either but do have bronchitis, the probability is 0.8. If they have neither and do not have bronchitis, the probability is 0.1.\n\n\\[ P(\\text{dyspnoea} = 1 \\mid \\text{either} = 1, \\text{bronchitis} = 1) = 0.9 \\] \\[ P(\\text{dyspnoea} = 1 \\mid \\text{either} = 1, \\text{bronchitis} = 0) = 0.7 \\] \\[ P(\\text{dyspnoea} = 1 \\mid \\text{either} = 0, \\text{bronchitis} = 1) = 0.8 \\] \\[ P(\\text{dyspnoea} = 1 \\mid \\text{either} = 0, \\text{bronchitis} = 0) = 0.1 \\]"
  },
  {
    "objectID": "blog/exploring-expert-systems/index.html#simulating",
    "href": "blog/exploring-expert-systems/index.html#simulating",
    "title": "Exploring expert systems",
    "section": "Simulating",
    "text": "Simulating\nI remembered an old WinBUGS simulation which used this model. The WinBUGS documentation was hard to find, but luckily @chjackson has mirrored it on GitHub. Unfortunately, this example is missing from the example models Stan repository.\nThe WinBUGS simulation chooses to specify each trait as a categorical variable with two categories, instead of modelling them as Bernoulli random variables - I’d guess to make it easier to specify the conditional probabilities.\nmodel\n{\n    smoking ~ dcat(p.smoking[1:2])\n    tuberculosis ~ dcat(p.tuberculosis[asia,1:2])\n    lung.cancer ~ dcat(p.lung.cancer[smoking,1:2])\n    bronchitis ~ dcat(p.bronchitis[smoking,1:2])\n    either &lt;- max(tuberculosis,lung.cancer)\n    xray ~ dcat(p.xray[either,1:2])\n    dyspnoea ~ dcat(p.dyspnoea[either,bronchitis,1:2])\n}\nIn this example, all parameters are known and included as data, so there are no unknown probabilistic parameters like in a typical Stan simulation. The Stan guidance has a section on sampling without parameters which shows how we can use a generated quantities block to create data based on the model.\n\ndata {\n  real&lt;lower=0, upper=1&gt; p_asia;                  // 0.01\n  real&lt;lower=0, upper=1&gt; p_smoking;               // 0.5\n\n  // The following are the conditional probabilities\n  array[2] real&lt;lower=0, upper=1&gt; p_tuberculosis; // 0.01, 0.05\n  array[2] real&lt;lower=0, upper=1&gt; p_lung_cancer;  // 0.01, 0.1\n  array[2] real&lt;lower=0, upper=1&gt; p_bronchitis;   // 0.3, 0.6\n  array[2] real&lt;lower=0, upper=1&gt; p_xray;         // 0.05, 0.98\n  array[2, 2] real&lt;lower=0, upper=1&gt; p_dyspnoea;  // 0.1, 0.8, 0.7, 0.9\n}\n\ngenerated quantities {\n  int&lt;lower=0, upper=1&gt; asia = bernoulli_rng(p_asia);\n  int&lt;lower=0, upper=1&gt; smoking = bernoulli_rng(p_smoking);\n  int&lt;lower=0, upper=1&gt; tuberculosis = bernoulli_rng(p_tuberculosis[asia + 1]);\n  int&lt;lower=0, upper=1&gt; lung_cancer = bernoulli_rng(p_lung_cancer[smoking + 1]);\n  int&lt;lower=0, upper=1&gt; bronchitis = bernoulli_rng(p_bronchitis[smoking + 1]);\n  int&lt;lower=0, upper=1&gt; either = max(tuberculosis, lung_cancer);\n  int&lt;lower=0, upper=1&gt; xray = bernoulli_rng(p_xray[either + 1]);\n  int&lt;lower=0, upper=1&gt; dyspnoea = bernoulli_rng(p_dyspnoea[either + 1, bronchitis + 1]);\n}\n\nWe use indexing to retrieve the appropriate conditional probability at each step. For example, if asia == 0 then tuberculosis = bernoulli_rng(p_tuberculosis[1]), but if asia == 1 then tuberculosis = bernoulli_rng(p_tuberculosis[2]).\nWe run the model with rstan to generate our samples.\n\nlibrary(rstan)\nlibrary(dplyr)\nlibrary(tidyr)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nset.seed(123)\n\ndata_list &lt;- list(\n  p_asia         = 0.01,\n  p_smoking      = 0.5,\n  p_tuberculosis = c(0.01, 0.05),\n  p_lung_cancer  = c(0.01, 0.1),\n  p_bronchitis   = c(0.3, 0.6),\n  p_xray         = c(0.05, 0.98),\n  p_dyspnoea     = matrix(c(0.1, 0.8, 0.7, 0.9), nrow = 2, byrow = TRUE)\n)\n\nfit &lt;- sampling(\n  model,\n  data = data_list,\n  iter = 100000,\n  chains = 4,\n  algorithm = \"Fixed_param\" # Needed for sampling without parameters\n)\n\nprint(fit)\n\nThe simulation ran successfully, and the mean column of the summary gives the marginal probabilities for each of the traits in the system. Let’s take a look at our samples:\n\nparams &lt;- rstan::extract(fit)\nsamples &lt;- tibble::as_tibble(params)\n\nprint(samples)\n#&gt; # A tibble: 200,000 × 9\n#&gt;     asia smoking tuberculosis lung_cancer bronchitis either  xray dyspnoea  lp__\n#&gt;    &lt;dbl&gt; &lt;dbl[1&gt;    &lt;dbl[1d]&gt;   &lt;dbl[1d]&gt;  &lt;dbl[1d]&gt; &lt;dbl[&gt; &lt;dbl&gt; &lt;dbl[1d&gt; &lt;dbl&gt;\n#&gt;  1     0       0            0           0          0      0     0        0     0\n#&gt;  2     0       1            0           0          1      0     0        1     0\n#&gt;  3     0       1            0           0          0      0     0        0     0\n#&gt;  4     0       0            0           0          0      0     0        0     0\n#&gt;  5     0       1            0           0          1      0     0        0     0\n#&gt;  6     0       1            0           0          1      0     0        1     0\n#&gt;  7     0       1            0           0          1      0     0        0     0\n#&gt;  8     0       1            0           0          0      0     0        0     0\n#&gt;  9     0       0            0           0          0      0     0        0     0\n#&gt; 10     0       0            0           0          0      0     0        0     0\n#&gt; # ℹ 199,990 more rows"
  },
  {
    "objectID": "blog/exploring-expert-systems/index.html#answering-some-questions",
    "href": "blog/exploring-expert-systems/index.html#answering-some-questions",
    "title": "Exploring expert systems",
    "section": "Answering some questions",
    "text": "Answering some questions\nSo now we have some simulated data, we can estimate probabilities by looking at the proportion of samples which meet certain conditions. First, let’s do some sense checks to make sure our simulated data gives results we’d expect from the model specification.\nThe model specification says the probability of someone having visited Asia is 0.01, and the probability that someone smokes is 0.5. Our samples agree with this.\n\nsamples |&gt;\n  summarise(\n    asia_prob = mean(asia),\n    smoking_prob = mean(smoking)\n  )\n#&gt; # A tibble: 1 × 2\n#&gt;   asia_prob smoking_prob\n#&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1   0.00952        0.504\n\nTo check another, the model specification says that if someone smokes, the probability they have lung cancer is 0.1. If they do not smoke, the probability is 0.01. Our samples agree with this too, so it looks like our simulation has behaved as expected.\n\nsamples |&gt;\n  group_by(smoking) |&gt;\n  summarise(lung_cancer_prob = mean(lung_cancer))\n#&gt; # A tibble: 2 × 2\n#&gt;     smoking lung_cancer_prob\n#&gt;   &lt;dbl[1d]&gt;            &lt;dbl&gt;\n#&gt; 1         0          0.00957\n#&gt; 2         1          0.101\n\nNow we can answer the questions from the paper:\n\nA patient presents at a chest clinic with dyspnoea, and has recently visited Asia. Smoking history and chest X-ray are not yet available. The doctor would like to know the chance that each of the diseases is present.\n\nHere we’ll want to filter to samples where asia == 1 and dyspnoea == 1, and to look at the occurrence of each of the other traits.\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1) |&gt;\n  # Make each instance of a disease appear on its own line\n  pivot_longer(cols = c(-asia,-dyspnoea), names_to = \"trait\") |&gt;\n  group_by(trait) |&gt;\n  summarise(probability = mean(value))\n#&gt; # A tibble: 6 × 2\n#&gt;   trait        probability\n#&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 bronchitis        0.803 \n#&gt; 2 either            0.176 \n#&gt; 3 lung_cancer       0.0873\n#&gt; 4 smoking           0.643 \n#&gt; 5 tuberculosis      0.0931\n#&gt; 6 xray              0.217\n\nFrom this we can see it’s likely that the patient has bronchitis, and less likely that they have lung cancer or tuberculosis. It’s quite likely that the patient smokes, and less likely that a positive x-ray would be seen. The probabilities agree with what was found in the WinBUGS simulation (which are computed by taking one away from the mean in the reported table).\n\n…if tuberculosis were ruled out by another test, how would that change the belief in lung cancer?\n\nHere we filter for samples where tuberculosis == 0 too.\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1, tuberculosis == 0) |&gt;\n  pivot_longer(cols = c(-asia,-dyspnoea), names_to = \"trait\") |&gt;\n  group_by(trait) |&gt;\n  summarise(probability = mean(value)) |&gt;\n  filter(trait == \"lung_cancer\")\n#&gt; # A tibble: 1 × 2\n#&gt;   trait       probability\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1 lung_cancer      0.0911\n\nThe probability of lung cancer once we’ve ruled out tuberculosis hasn’t changed by much.\n\nAlso, would knowing smoking history or getting an X-ray contribute most information about cancer, given that smoking may ‘explain away’ the dyspnoea since bronchitis is considered a possibility?\n\nGoing back to the case where asia == 1 and dyspnoea == 1, we can look at different cases for smoking history and x-ray results to see how that affects the probability of lung cancer.\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1) |&gt;\n  group_by(smoking) %&gt;%\n  summarise(lung_cancer_prob = mean(lung_cancer))\n#&gt; # A tibble: 2 × 2\n#&gt;     smoking lung_cancer_prob\n#&gt;   &lt;dbl[1d]&gt;            &lt;dbl&gt;\n#&gt; 1         0           0.0130\n#&gt; 2         1           0.129\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1) |&gt;\n  group_by(xray) %&gt;%\n  summarise(lung_cancer_prob = mean(lung_cancer))\n#&gt; # A tibble: 2 × 2\n#&gt;        xray lung_cancer_prob\n#&gt;   &lt;dbl[1d]&gt;            &lt;dbl&gt;\n#&gt; 1         0            0    \n#&gt; 2         1            0.403\n\nHere we can see that the x-ray result gives a stronger signal for the presence (or absence) of lung cancer presence than smoking history.\n\nFinally, when all information is in, can we identify which was the most influential in forming our judgement?\n\nSo given asia == 1 and dyspnoea == 1, we can look at the samples for different known smoking histories and different xray results and see what they do to the probabilities for each of the diseases.\n\nsamples |&gt;\n  select(-lp__) |&gt;\n  filter(asia == 1, dyspnoea == 1) |&gt;\n  group_by(smoking, xray) |&gt;\n  summarise(\n    tuberculosis_prob = mean(tuberculosis),\n    lung_cancer_prob = mean(lung_cancer),\n    bronchitis_prob = mean(bronchitis)\n  ) |&gt;\n  arrange(xray)\n#&gt; `summarise()` has grouped output by 'smoking'. You can override using the\n#&gt; `.groups` argument.\n#&gt; # A tibble: 4 × 5\n#&gt; # Groups:   smoking [2]\n#&gt;     smoking      xray tuberculosis_prob lung_cancer_prob bronchitis_prob\n#&gt;   &lt;dbl[1d]&gt; &lt;dbl[1d]&gt;             &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1         0         0           0.00400           0                0.772\n#&gt; 2         1         0           0.00236           0                0.915\n#&gt; 3         0         1           0.667             0.0702           0.404\n#&gt; 4         1         1           0.310             0.550            0.674\n\nFrom these results, we see that a positive x-ray increases the probability of lung cancer and tuberculosis, regardless of smoking status, while smoking increases bronchitis likelihood regardless of the x-ray result. The x-ray result appears to be the most influential additional information here, as it causes the largest changes to probabilities of the different diseases."
  },
  {
    "objectID": "blog/qualify-in-sql/index.html",
    "href": "blog/qualify-in-sql/index.html",
    "title": "QUALIFY in SQL",
    "section": "",
    "text": "Last year, while starting to use Snowflake, I discovered the QUALIFY keyword in SQL.\nQUALIFY filters results based on a window function. It’s not part of the SQL standard and so won’t be supported in all SQL engines, but I think it has good coverage in some of the newer ones.\nLet’s say we have an employees table, courtesy of ChatGPT:\n┌────┬─────────┬─────────────┬────────┐\n│ id ┆ name    ┆ department  ┆ salary │\n╞════╪═════════╪═════════════╪════════╡\n│  1 ┆ Alice   ┆ Engineering ┆  70000 │\n│  2 ┆ Bob     ┆ Engineering ┆  72000 │\n│  3 ┆ Charlie ┆ HR          ┆  65000 │\n│  4 ┆ Diana   ┆ HR          ┆  68000 │\n│  5 ┆ Eve     ┆ Engineering ┆  71000 │\n└────┴─────────┴─────────────┴────────┘\nWe want to find the top earner for each department. With QUALIFY, the query becomes concise:\nSELECT * \nFROM employees\nQUALIFY RANK() OVER (PARTITION BY department ORDER BY salary DESC) = 1;\nHere, RANK() ranks employees within each department by salary, and QUALIFY filters the results to show only the top earner in each department.\n┌────┬───────┬─────────────┬────────┐\n│ id ┆ name  ┆ department  ┆ salary │\n╞════╪═══════╪═════════════╪════════╡\n│  2 ┆ Bob   ┆ Engineering ┆  72000 │\n│  4 ┆ Diana ┆ HR          ┆  68000 │\n└────┴───────┴─────────────┴────────┘\nPreviously when doing something like this, I would use a common table expression (CTE):\nWITH ranked_employees AS (\n    SELECT \n        *,\n        RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS employee_rank\n    FROM employees\n)\n\nSELECT *\nFROM ranked_employees\nWHERE employee_rank = 1;\nAs a small query, I still think using a CTE is fine, but when using multiple CTEs in a larger or more complex query, the brevity of QUALIFY is really nice.\nWant to try it out? Check out the DuckDB web shell."
  }
]